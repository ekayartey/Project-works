{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating similarity using MinHash and MapReduce\n",
    "\n",
    "Now we are going to reuse the code to download Wikipedia articles, and then we will use MinHash to estimate the Jaccard similarity between those articles. To do things properly, we are going to tokenise and lemmatise the articles using NLTK (an Natural Processing Language library).\n",
    "\n",
    "We will develop this as a MapReduce task.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tokenize\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_article(wiki_article, timeout=10):\n",
    "    '''\n",
    "    Method that gets a Wikipedia page. \n",
    "    '''\n",
    "    wiki_page_url = 'https://en.wikipedia.org/wiki/'+wiki_article\n",
    "    response = requests.get(url=wiki_page_url, timeout=timeout)\n",
    "    article = response.text\n",
    "    return article\n",
    "\n",
    "\n",
    "def get_wikipedia_articles(wiki_articles):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        futures = []\n",
    "        for article in wiki_articles:\n",
    "            futures.append(executor.submit(get_wikipedia_article,\n",
    "                                           wiki_article=article)\n",
    "                          )\n",
    "        articles = []\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            articles.append(future.result())\n",
    "    return articles\n",
    "\n",
    "\n",
    "# Let's define some functions to clean HTML code, punctuation marks and symbols from the original articles\n",
    "def clean_html(raw_html):\n",
    "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    cleantext = re.sub(cleanr, ' ', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "def clean_punctuation_marks(text):\n",
    "    return re.sub(r'[^\\w\\s]',' ', text)\n",
    "\n",
    "def clean_return_and_tabs(text):\n",
    "    return re.sub(r'[\\n, \\t]',' ', text)\n",
    "\n",
    "def clean_article(data):\n",
    "    cdata = clean_html(data)\n",
    "    cdata = clean_punctuation_marks(cdata)\n",
    "    cdata = clean_return_and_tabs(cdata)\n",
    "    return cdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now it is time to fetch some concepts\n",
    "\n",
    "We will download the Wikipedia articles specified in the list below. After you run the simulation for the first time, feel free to modify them to test with other terms!\n",
    "\n",
    "We will run the simulation with 'lion', 'beetle' and 'tiger'. Which ones do you think should have a greater similarity? Well both lions and tigers are large felines while the beetle is an insect..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_articles = [\n",
    "    'lion',\n",
    "    'beetle',\n",
    "    'tiger',\n",
    "]\n",
    "\n",
    "# Other examples: \n",
    "# (I am intentionally putting a few concepts that should be related between them and one that is not, \n",
    "# but feel free to put whatever you want in there, as long as there is a Wikipedia article for it)\n",
    "\n",
    "# wiki_articles = ['Ford', 'Audi', 'Mercedes-Benz', 'lamb']\n",
    "# wiki_articles = ['Donald_Trump', 'Barack_Obama', 'computer', 'Bill_Clinton']\n",
    "\n",
    "articles = get_wikipedia_articles(wiki_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have the articles, let's do some basic cleaning and NLP on them:\n",
    "\n",
    "Basic cleaning -> tokenise -> remove stopwords -> stem -> remove numbers -> remove specific stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = tokenize.NLTKWordTokenizer()\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we make it all lowercase and do a basic cleaning as we did in previous interactive activities\n",
    "articles = map(lambda a: a.lower(), articles)\n",
    "articles = map(clean_article, articles)\n",
    "# Then we tokenise the articles, extracing the words from them\n",
    "articles = map(tokeniser.tokenize, articles)\n",
    "\n",
    "# We will also remove the English stopwords \n",
    "# (you can check which ones are the English stopwords by checking this: print(stopwords.words('english')))\n",
    "\n",
    "stopwords = set(['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', \n",
    "                'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', \n",
    "                'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', \n",
    "                'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', \n",
    "                'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', \n",
    "                'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', \n",
    "                'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', \n",
    "                'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', \n",
    "                'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', \n",
    "                'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', \n",
    "                'doing', 'it', 'how', 'further', 'was', 'here', 'than'])\n",
    "\n",
    "def remove_stopwords(document, stopwords):\n",
    "    output_doc = []\n",
    "    for word in document:\n",
    "        if word not in stopwords:\n",
    "            output_doc.append(word)\n",
    "    return output_doc\n",
    "\n",
    "articles = map(lambda a: remove_stopwords(a, stopwords), articles)\n",
    "\n",
    "# And then we stem those words to remove plurals and other variations\n",
    "articles = [map(stemmer.stem, article) for article in articles]\n",
    "\n",
    "\n",
    "def remove_numbers(document):\n",
    "    numbers = set(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n",
    "    output_doc = []\n",
    "    for word in document:\n",
    "        if not any(num in word for num in numbers):\n",
    "            output_doc.append(word)\n",
    "    return output_doc\n",
    "\n",
    "articles = map(remove_numbers, articles)\n",
    "\n",
    "def remove_wiki_and_wg_words(document):\n",
    "    output_doc = []\n",
    "    for word in document:\n",
    "        if not word.startswith('wg') and not word.startswith('wiki'):\n",
    "            output_doc.append(word)\n",
    "    return output_doc\n",
    "\n",
    "articles = map(remove_wiki_and_wg_words, articles)\n",
    "articles = list(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Jaccard similarity:\n",
    "\n",
    "We can now use those cleaned articles to calculate their real Jaccard similarities. We can do this because we are just dealing with a few articles and they are relatively short documents. If we had to do this for all websites on the internet, or even just a few tens of thousands of articles, it wouldn't be feasible as it is.\n",
    "\n",
    "You should see that the Tiger and Lion articles should be more similar between them than with the Beetle one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(a1, a2):\n",
    "    set1 = set(a1)\n",
    "    set2 = set(a2)\n",
    "    union_size = len(set1.union(set2))\n",
    "    intersection_size = len(set1.intersection(set2))\n",
    "    return round(intersection_size/union_size, 4)\n",
    "\n",
    "for i in range(len(articles)-1):\n",
    "    for j in range(i+1, len(articles)):\n",
    "        print('Similarity between', articles[i][0], 'and', articles[j][0], ':',jaccard(articles[i], articles[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinHash:\n",
    "\n",
    "Now let's estimate those real Jaccard measures with the MinHash algorithm we saw in the session and see how good our results are...\n",
    "\n",
    "First we will create 3-word shingles (3-grams) and hash them for all of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import binascii  # To transform (hash) each shingle to a 32-bit integer \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingle_article(article, shingle_size=3):\n",
    "    hashed_shingles = [] # Shingles are those 3 words transformed in an integer using CRC32 algorithm\n",
    "    for i in range(len(article)-(shingle_size-1)):\n",
    "        # We put groups (shingles) of 3 words together, separated by a space\n",
    "        # We could change the number of words per shingle, changing shingle_size :)\n",
    "        shingle = ' '.join([article[i+j] for j in range(shingle_size)])\n",
    "        crc_hash = binascii.crc32(bytes(shingle.encode('UTF-8')))\n",
    "        hashed_shingles.append(crc_hash)\n",
    "    return hashed_shingles\n",
    "\n",
    "SHINGLE_SIZE = 2\n",
    "\n",
    "# We \"save\" the first word of the article in order to be able to idenfity it later when we hash it all \n",
    "shingled_articles = map(lambda a: (a[0], shingle_article(a, shingle_size=SHINGLE_SIZE)), articles)\n",
    "shingled_articles = list(shingled_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxCoefficient = 2**32-1  # Because we are transforming shingles into 32-bit integers, we know their maximum\n",
    "big_prime = 4294967311  # And we found their next primer in internet (when you work with 32-bit ints take that number)\n",
    "\n",
    "\n",
    "N = 20 # Using 20 hashing functions, feel free to modify!\n",
    "\n",
    "# We will use N hashing functions, so we need to randomly take N As and N Bs for our N hashing functions\n",
    "# Our hashing functions will be of the form: hashed_value = A * value + B % big_prime\n",
    "a_coeffs = [random.randint(0, maxCoefficient) for i in range(N)]\n",
    "b_coeffs = [random.randint(0, maxCoefficient) for i in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashed_value(shingle, a_coeff, b_coeff, big_prime):\n",
    "    return ((shingle * a_coeff) + b_coeff) % big_prime\n",
    "\n",
    "# For each article, and each of the N hash functions, we calculate the hashes of all the shingles\n",
    "# and get the minimum of those hashes. This will create a signature of size N for each document. \n",
    "# Finally, to estimate their Jaccard similarity, we just only compare the signatures\n",
    "article_signatures = {}\n",
    "article_names = []\n",
    "for article_name, shingled_article in shingled_articles:\n",
    "    article_signature = []\n",
    "    for hashing_function_index in range(N):\n",
    "        a_coeff = a_coeffs[hashing_function_index]\n",
    "        b_coeff = b_coeffs[hashing_function_index]\n",
    "        hashes = map(lambda shingles: get_hashed_value(shingles, a_coeff, b_coeff, big_prime), shingled_article)\n",
    "        article_signature.append(min(list(hashes)))\n",
    "    article_signatures[article_name] = article_signature\n",
    "    article_names.append(article_name)\n",
    "    \n",
    "for s in article_signatures:\n",
    "    print('Signature for', s,': ',article_signatures[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Article signatures:\n",
    "\n",
    "You can have a look at article_signatures just printed above to see what we are actually using to compare the articles. We have reduced documents of potentially thousands of words to just N numbers. \n",
    "\n",
    "We will apply Jaccard on those N numbers, and still lion and tiger should be more similar than lion and bettle or tiger and beetle.\n",
    "\n",
    "In the code cells above, feel free to modify the number of hashes we are using, as well as the shingle_size (how many words per shingle).\n",
    "\n",
    "\n",
    "* What happens if you put a large shingle size? (for example 20 words). Why do you think that is?\n",
    "* And when we increase/decrease the number of hashing functions? Is the more always the better?\n",
    "* Can you create a situation where the Beetle article is more similar to lion or tiger than between them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using', N, 'hashing functions, and shingles of size',SHINGLE_SIZE, ', MinHas produces the following results:')\n",
    "for i in range(len(article_names)-1):\n",
    "    for j in range(i+1, len(article_names)):\n",
    "        print('Similarity between', article_names[i], 'and', article_names[j], ':',\n",
    "              jaccard(article_signatures[article_names[i]], article_signatures[article_names[j]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Exercises\n",
    "\n",
    "This week's exercises are just to continue playing with the simulations. For example:\n",
    "\n",
    "* Can you explain why MinHash is a good algorithm for detecting plagiarism? How would you set the parameters of (1) shingling size and (2) number of hash functions to tune it for plagiarism detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
