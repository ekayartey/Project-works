{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Working with numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_integer_array(n=20000, max_n=100):\n",
    "    random_array = []\n",
    "    # The function range(a, b) produces an array [a, a+1, a+2, ..., b-2, b-1]\n",
    "    for r in range(0, n): # Iterating between 0 and n.\n",
    "        random_number = random.randint(0, max_n) # Produces a random int between 0 and max_n\n",
    "        random_array.append(random_number)\n",
    "    return random_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are calling the function with the default parameters (20k and 100), but feel free to change that.\n",
    "random_array = create_random_integer_array() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total average is: 50.3681\n"
     ]
    }
   ],
   "source": [
    "# To calculate the average, we need to count the elements and sum them. Both of these operations are very fast\n",
    "# and optimised in Python, so even if it is a large array, it will still be faster than a parallel solution,\n",
    "# but let's design what this parallel solution should look like if the number of elements couldn't fit in a \n",
    "# single computer and we needed a true distributed solution instead of a fake parallel one like in this example\n",
    "\n",
    "# A non-parallel solution is just:\n",
    "average = sum(random_array) / len(random_array)\n",
    "print('The total average is:',average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part has: 4000 elements, summing 203658\n",
      "Part has: 4000 elements, summing 199859\n",
      "Part has: 4000 elements, summing 200720\n",
      "Part has: 4000 elements, summing 201541\n",
      "Part has: 4000 elements, summing 201584\n",
      "The total average is: 50.3681\n"
     ]
    }
   ],
   "source": [
    "# To test a \"fake parallel\" approach, we can divide the array in 5 parts for example, as in the code below \n",
    "# Feel free to change n_parts, the final solution should be the same, only that the calculation more parallel.\n",
    "n_parts = 5\n",
    "part_size = int(len(random_array) / n_parts)\n",
    "\n",
    "parts = []  # An array of arrays, containing the n_parts\n",
    "for p in range(n_parts):\n",
    "    part = random_array[part_size*p: part_size*(p+1)]\n",
    "    parts.append(part)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# Then apply a MapReduce approach. For that we define two functions, the first one for the map stage, \n",
    "# and the second one for the reduce stage\n",
    "\n",
    "def get_count_and_sum(array):\n",
    "    '''\n",
    "    Function that returns a dictionary with two values: the count and the sum of the array given\n",
    "    as an input parameter\n",
    "    '''\n",
    "    return {\n",
    "        'count': len(array),\n",
    "        'sum': sum(array)\n",
    "    }\n",
    "\n",
    "def reduce_sum_counts(sum_count_arrays):\n",
    "    '''\n",
    "    Function that takes the dictionaries calculated before in the map stage, and aggregates them\n",
    "    summing all of the 'count' fields and all of the 'sum' fields, and returning these two values\n",
    "    '''\n",
    "    total_count = 0\n",
    "    total_sum = 0\n",
    "    for s in sum_counts:\n",
    "        total_count += s['count']\n",
    "        total_sum += s['sum']\n",
    "    return total_count, total_sum\n",
    "\n",
    "# MAP PART\n",
    "# We call the Python's native map function which accepts two parameters: a function (our map function) and \n",
    "# an array. This map native function applies the map function parameter to each element in the 2nd parameter (the array) \n",
    "# In short: For each of the array parts, we calculate their sum and length.\n",
    "sum_counts = map(get_count_and_sum, parts)\n",
    "# We cast the map object (which is a generator) as a list. This is the same as iterating until the generator is over\n",
    "# and add each element to a list - casting it is simpler.\n",
    "sum_counts = list(sum_counts)  \n",
    "# We print out the partial results of the map stage\n",
    "for s in sum_counts:\n",
    "    print('Part has:',s['count'], 'elements, summing',s['sum'])\n",
    "\n",
    "# REDUCE PART\n",
    "count, summ = reduce_sum_counts(sum_counts)\n",
    "# After we have aggregated everything, we just have the final two parameters (the total sum and length of the array)\n",
    "# so the average is just sum over count:\n",
    "average = summ/count\n",
    "\n",
    "print('The total average is:',average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sum_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Counting a word in Wikipedia articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def count_term_in_wikipedia_article(term, wiki_article, timeout=10):\n",
    "    '''\n",
    "    Method that counts how many times the term \"term\" appears in the \n",
    "    Wikipedia page \"wiki_article\". \n",
    "    It also prints in the screen the result before returning it. \n",
    "    '''\n",
    "    wiki_page_url = 'https://en.wikipedia.org/wiki/'+wiki_article\n",
    "    response = requests.get(url=wiki_page_url, timeout=timeout)\n",
    "    article = response.text\n",
    "    count = article.count(term)\n",
    "    print('\"', term, '\" appears', count, 'times in the article',wiki_article,'\\n')\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 'data'\n",
    "wiki_articles = [\n",
    "    'big_data', \n",
    "    'data_analysis', \n",
    "    'data_modelling', \n",
    "    'relational_algebra', \n",
    "    'software_engineering',\n",
    "    'machine_learning',\n",
    "    'artificial_intelligence',\n",
    "    'support_vector_machine',\n",
    "    'random_forest',\n",
    "    'logistic_regression',\n",
    "    'naive_bayes_classifier',\n",
    "    'bayesian_probability',\n",
    "    'database',\n",
    "    'data',\n",
    "    'information',\n",
    "    'entity_relationship',\n",
    "    'information_system',\n",
    "    'information_technology',\n",
    "    'apache_hadoop',\n",
    "    'apache_spark',\n",
    "    'array_data_structure',\n",
    "    'dictionary_data_structure',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing tasks one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" data \" appears 1284 times in the article big_data \n",
      "\n",
      "\" data \" appears 553 times in the article data_analysis \n",
      "\n",
      "\" data \" appears 266 times in the article data_modelling \n",
      "\n",
      "\" data \" appears 197 times in the article relational_algebra \n",
      "\n",
      "\" data \" appears 174 times in the article software_engineering \n",
      "\n",
      "\" data \" appears 371 times in the article machine_learning \n",
      "\n",
      "\" data \" appears 536 times in the article artificial_intelligence \n",
      "\n",
      "\" data \" appears 146 times in the article support_vector_machine \n",
      "\n",
      "\" data \" appears 104 times in the article random_forest \n",
      "\n",
      "\" data \" appears 205 times in the article logistic_regression \n",
      "\n",
      "\" data \" appears 66 times in the article naive_bayes_classifier \n",
      "\n",
      "\" data \" appears 98 times in the article bayesian_probability \n",
      "\n",
      "\" data \" appears 930 times in the article database \n",
      "\n",
      "\" data \" appears 241 times in the article data \n",
      "\n",
      "\" data \" appears 153 times in the article information \n",
      "\n",
      "\" data \" appears 251 times in the article entity_relationship \n",
      "\n",
      "\" data \" appears 159 times in the article information_system \n",
      "\n",
      "\" data \" appears 195 times in the article information_technology \n",
      "\n",
      "\" data \" appears 38 times in the article apache_hadoop \n",
      "\n",
      "\" data \" appears 199 times in the article apache_spark \n",
      "\n",
      "\" data \" appears 252 times in the article array_data_structure \n",
      "\n",
      "\" data \" appears 89 times in the article dictionary_data_structure \n",
      "\n",
      "The process took 20.264259848976508 seconds\n",
      "Total appearances of the term data : 6507\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "total_appearances = 0\n",
    "for article in wiki_articles:\n",
    "    total_appearances += count_term_in_wikipedia_article(term, article)\n",
    "stop = time.perf_counter()\n",
    "print('The process took', stop-start, 'seconds')\n",
    "print('Total appearances of the term', term, ':', total_appearances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing tasks concurrently imitating a MapReduce process\n",
    "Only that it is a parallel approach, not a distributed one yet... So the MapReduce process is actually running over different threads in the machine, not distributed. \n",
    "\n",
    "What happens if you change the number of MAX_WORKERS?\n",
    "\n",
    "If you manage to find it, why do you think that is the optimal number? Why doesn't it always works best the more workers we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" data \" appears 66 times in the article naive_bayes_classifier \n",
      "\n",
      "\" data \" appears 553 times in the article data_analysis \n",
      "\n",
      "\" data \" appears 266 times in the article data_modelling \"\" \n",
      " data\n",
      "data  \" appears \" \" data \"\" appears data \" appears 197 \" data times in the articledata \" appears 1284  \" appears 153 times in the article information \n",
      "\n",
      "times in the article big_data \n",
      "\n",
      "\" appears104 relational_algebra241 times in the article data \n",
      "\n",
      " 98 times in the article bayesian_probability \n",
      "\n",
      "  \"\n",
      "\n",
      " times in the article datarandom_forest  \" appears 174 times in the article software_engineering \n",
      "\n",
      "\n",
      "\n",
      "\" data\" \" appears  146 times in the article support_vector_machine \n",
      "\n",
      "data \" appears 930 times in the article database \n",
      "\n",
      "\" data \" appears 371 times in the article machine_learning \n",
      "\n",
      "\" data \" appears 205 times in the article logistic_regression \n",
      "\n",
      "\" data \" appears 536 times in the article artificial_intelligence \n",
      "\n",
      "\" data \" appears 251 times in the article entity_relationship \n",
      "\n",
      "\" data \" appears 89 times in the article dictionary_data_structure \" data \" appears 38 \n",
      "times in the article apache_hadoop\n",
      " \n",
      "\n",
      "\" data \" appears 159 times in the article information_system \n",
      "\n",
      "\" data \" appears 199 times in the article apache_spark \n",
      "\n",
      "\" data \" appears 195 times in the article information_technology \n",
      "\n",
      "\" data \" appears 252 times in the article array_data_structure \n",
      "\n",
      "The process took 0.579772467026487 seconds\n",
      "Total appearances of the term data : 6507\n"
     ]
    }
   ],
   "source": [
    "MAX_WORKERS = 15\n",
    "\n",
    "start = time.perf_counter()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = []\n",
    "    \n",
    "    # MAP STAGE - That \"submit\" function is basically a map through all of the parallel processes \n",
    "    for article in wiki_articles:\n",
    "        futures.append(executor.submit(count_term_in_wikipedia_article, \n",
    "                                       term=term, \n",
    "                                       wiki_article=article)\n",
    "                      )\n",
    "    \n",
    "    # SHUFFLE AND SORT STAGE\n",
    "    appearances = []\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        appearances.append(future.result())\n",
    "        \n",
    "    # REDUCE STAGE\n",
    "    total_appearances_concurrent = sum(appearances)\n",
    "\n",
    "stop = time.perf_counter()\n",
    "print('The process took', stop-start, 'seconds')\n",
    "print('Total appearances of the term', term, ':', total_appearances_concurrent)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
