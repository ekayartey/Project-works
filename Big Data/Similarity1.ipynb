{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sqrt\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pearsonr, spearmanr\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_iris\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_iris' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m iris_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_iris\u001b[49m(return_X_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, as_frame\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m iris \u001b[38;5;241m=\u001b[39m iris_data[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m iris\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msepal_length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msepal_width\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpetal_length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpetal_width\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_iris' is not defined"
     ]
    }
   ],
   "source": [
    "iris_data = load_iris(return_X_y=True, as_frame=True)\n",
    "iris = iris_data[0]\n",
    "iris.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "iris['type'] = iris_data[1]\n",
    "del iris_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample data: Iris dataset\n",
    "\n",
    "The Iris Dataset we just loaded with the above code consists of 150 records describing the measurements of iris flowers. Then we have an output named \"type\" which specifies the type of the iris flower (for simplicity, the names of those types were replaced by the numbers 0, 1 and 2). There are 50 records of each type of flower.\n",
    "\n",
    "Using the similarity functions we saw in the session, we are going to develop our own K-nearest neighbours algorithm to predict the flower types, given just their measurements. For that, we are going to take a random sample of 75 records (test dataset) and try to predict them by looking at their most similar measures from the remaining 75 records (train dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(iris, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test data has 75 records\n",
      "The train data has 75 records\n"
     ]
    }
   ],
   "source": [
    "print('The test data has', len(test), 'records')\n",
    "print('The train data has', len(train), 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width  type\n",
       "56            6.3          3.3           4.7          1.6     1\n",
       "37            4.9          3.6           1.4          0.1     0\n",
       "70            5.9          3.2           4.8          1.8     1\n",
       "21            5.1          3.7           1.5          0.4     0\n",
       "33            5.5          4.2           1.4          0.2     0\n",
       "..            ...          ...           ...          ...   ...\n",
       "82            5.8          2.7           3.9          1.2     1\n",
       "87            6.3          2.3           4.4          1.3     1\n",
       "12            4.8          3.0           1.4          0.1     0\n",
       "111           6.4          2.7           5.3          1.9     2\n",
       "125           7.2          3.2           6.0          1.8     2\n",
       "\n",
       "[75 rows x 5 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the similarity functions\n",
    "\n",
    "As you can see, the 4 input parameters (sepal and petal lengths and widths) are numeric. Therefore, in order to compare our test records and find their most similar records in the train data, we will need a similarity function able to compare those numeric arrays of 4 elements each. In the next section we define some of the similarity functions we saw in our session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(array1, array2):\n",
    "    sq_sum_i = 0\n",
    "    for i, j in zip(array1, array2):\n",
    "        sq_sum_i += (i - j) ** 2\n",
    "    return sqrt(sq_sum_i)\n",
    "\n",
    "def manhattan_distance(array1, array2):\n",
    "    sum_i = 0\n",
    "    for i, j in zip(array1, array2):\n",
    "        sum_i += abs(i - j)\n",
    "    return sum_i\n",
    "\n",
    "def cosine_similarity(array1, array2):\n",
    "    mul_i = 0\n",
    "    array1_sq = 0\n",
    "    array2_sq = 0\n",
    "    for i, j in zip(array1, array2):\n",
    "        mul_i += i*j\n",
    "        array1_sq += i**2\n",
    "        array2_sq += j**2\n",
    "    return mul_i / (sqrt(array1_sq) * sqrt(array2_sq))\n",
    "\n",
    "\n",
    "# We will try some correlation between the numeric arrays as possible similarities also:\n",
    "def pearson(array1, array2):\n",
    "    corr, _ = pearsonr(array1, array2)\n",
    "    return corr if corr > 0 else 0\n",
    "\n",
    "def spearman(array1, array2):\n",
    "    corr, _ = spearmanr(array1, array2)\n",
    "    return corr if corr > 0 else 0\n",
    "\n",
    "# For the next similarity functions, we can establish a threshold to assess whether 2 values are considered the\n",
    "# same. In our case, we will set it by default to 0.2 (if the difference between 2 values is equals or less than\n",
    "# 0.2, then we consider them the same)\n",
    "\n",
    "def discrete_similarity(array1, array2, allowed_diff=0.2):\n",
    "    dist = 0\n",
    "    for i, j in zip(array1, array2):\n",
    "        # Discrete comparison, either 1 or 0. If one pais of elements is different, then it's zero, otherwise 1\n",
    "        if abs(i-j) > allowed_diff:\n",
    "            return 1  \n",
    "    return 0\n",
    "\n",
    "def hamming_distance(array1, array2, allowed_diff=0.2):\n",
    "    dist = 0\n",
    "    for i, j in zip(array1, array2):\n",
    "        if abs(i-j) > allowed_diff:\n",
    "            dist += 1\n",
    "    return dist\n",
    "\n",
    "def jaccard_similarity(array1, array2, allowed_diff=0.2):\n",
    "    intersection_set_size = 0\n",
    "    union_set_size = 0\n",
    "    for i, j in zip(array1, array2):\n",
    "        if abs(i-j) <= allowed_diff:\n",
    "            intersection_set_size += 1\n",
    "            union_set_size += 1\n",
    "        else:\n",
    "            union_set_size += 2\n",
    "    return intersection_set_size / union_set_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean (a, b): 6.48074069840786\n",
      "Euclidean (a, c): 1.4142135623730951\n",
      "Euclidean (b, c): 7.3484692283495345\n",
      "\n",
      "\n",
      "Manhattan (a, b): 10\n",
      "Manhattan (a, c): 2\n",
      "Manhattan (b, c): 12\n",
      "\n",
      "\n",
      "Cosine_similarity (a, b): 0.7003755189718213\n",
      "Cosine_similarity (a, c): 0.9978250097828444\n",
      "Cosine_similarity (b, c): 0.6629663121838063\n",
      "\n",
      "\n",
      "* We can see that correlations might work in some cases, but as with Cosine, careful with the magnitudes!\n",
      "Pearson_correlation (a, b): 0\n",
      "Pearson_correlation (a, c): 0.9819805060619655\n",
      "Pearson_correlation (b, c): 0\n",
      "\n",
      "\n",
      "Spearman_correlation (a, b): 0\n",
      "Spearman_correlation (a, c): 1.0\n",
      "Spearman_correlation (b, c): 0\n",
      "\n",
      "\n",
      "* And other similarities that also work with strings, and for which we put differences < 0.2 to be equal:\n",
      "discrete_similarity (a, b): 1\n",
      "discrete_similarity (a, c): 1\n",
      "discrete_similarity (b, c): 1\n",
      "\n",
      "\n",
      "hamming_distance (a, b): 3\n",
      "hamming_distance (a, c): 2\n",
      "hamming_distance (b, c): 3\n",
      "\n",
      "\n",
      "jaccard_similarity (a, b): 0.0\n",
      "jaccard_similarity (a, c): 0.2\n",
      "jaccard_similarity (b, c): 0.0\n"
     ]
    }
   ],
   "source": [
    "# Now we can use those functions. Using the examples from the lesson:\n",
    "a = [4, 6, 3]\n",
    "b = [3, 2, 8]\n",
    "c = [5, 7, 3]\n",
    "\n",
    "print('Euclidean (a, b):', euclidean_distance(a, b))\n",
    "print('Euclidean (a, c):', euclidean_distance(a, c))\n",
    "print('Euclidean (b, c):', euclidean_distance(b, c))\n",
    "print('\\n')\n",
    "print('Manhattan (a, b):', manhattan_distance(a, b))\n",
    "print('Manhattan (a, c):', manhattan_distance(a, c))\n",
    "print('Manhattan (b, c):', manhattan_distance(b, c))\n",
    "print('\\n')\n",
    "print('Cosine_similarity (a, b):', cosine_similarity(a, b))\n",
    "print('Cosine_similarity (a, c):', cosine_similarity(a, c))\n",
    "print('Cosine_similarity (b, c):', cosine_similarity(b, c))\n",
    "print('\\n')\n",
    "print('* We can see that correlations might work in some cases, but as with Cosine, careful with the magnitudes!')\n",
    "print('Pearson_correlation (a, b):', pearson(a, b))\n",
    "print('Pearson_correlation (a, c):', pearson(a, c))\n",
    "print('Pearson_correlation (b, c):', pearson(b, c))\n",
    "print('\\n')\n",
    "print('Spearman_correlation (a, b):', spearman(a, b))\n",
    "print('Spearman_correlation (a, c):', spearman(a, c))\n",
    "print('Spearman_correlation (b, c):', spearman(b, c))\n",
    "print('\\n')\n",
    "print('* And other similarities that also work with strings, and for which we put differences < 0.2 to be equal:')\n",
    "print('discrete_similarity (a, b):', discrete_similarity(a, b))\n",
    "print('discrete_similarity (a, c):', discrete_similarity(a, c))\n",
    "print('discrete_similarity (b, c):', discrete_similarity(b, c))\n",
    "print('\\n')\n",
    "print('hamming_distance (a, b):', hamming_distance(a, b))\n",
    "print('hamming_distance (a, c):', hamming_distance(a, c))\n",
    "print('hamming_distance (b, c):', hamming_distance(b, c))\n",
    "print('\\n')\n",
    "print('jaccard_similarity (a, b):', jaccard_similarity(a, b))\n",
    "print('jaccard_similarity (a, c):', jaccard_similarity(a, c))\n",
    "print('jaccard_similarity (b, c):', jaccard_similarity(b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's apply these similarities to find the K nearest neighbours of an array of input values.\n",
    "# Then, we will take the type of flower that appears the most in the K nearest neighbours as our prediction\n",
    "\n",
    "''' \n",
    "Our algorithm has 4 inputs:\n",
    " * test_array: In our case, the array of 4 values describing an iris flower, plus its real type, so we can compare \n",
    " * train_data: In our case, all our 120 records from which we will find the K most similar\n",
    " * comparative_function: the similarity or distance function we will use to know which train records are the nearest\n",
    " * similarity: a flag to know if comparative_function is a similarity or distance function. \n",
    "      If similarity, the higher the better; if distance, the lower the better\n",
    " * k: number of nearest neighbours we are going to use to make our prediction\n",
    " \n",
    " * RETURNS: Whether our prediction was correct or not.\n",
    "'''\n",
    "def predict_using_k_nearest_neighbours(test_array, train_data, comparative_function, similarity=True, k=10, print_results=False):\n",
    "    #Â We copy the input train_data, so we modify its copy, not the original\n",
    "    train_dataset = train_data.copy()\n",
    "    # The last value in the test array is the real flower type. We will use it to compare with our prediction\n",
    "    real_flower_type = test_array[-1] \n",
    "    # The other 4 values are the measure of the flower that we will use to find its K nearest neighbours\n",
    "    test_flower_measures = test_array[:4]\n",
    "    # And these 4 columns are the inputs we need to use to predict the flower type (we remove flower type from there)\n",
    "    input_columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "    train_dataset['measure'] = train_dataset.apply(\n",
    "        lambda row: comparative_function(test_flower_measures, row[input_columns]),\n",
    "        axis=1\n",
    "    )\n",
    "    if similarity: # If similarity, we sort the 120 records by their measure in descending order, and that 1st K\n",
    "        k_nearest = train_dataset.sort_values('measure', axis=0, ascending=False)[:k]\n",
    "    else: # Otherwise, it is a distance function, so the less the better, we take the smallest K\n",
    "        k_nearest = train_dataset.sort_values('measure', axis=0, ascending=True)[:k]\n",
    "    \n",
    "    # Now we select the flower type that is repeated the most in those K selected row. That is the mode.\n",
    "    predicted_flower_type = k_nearest['type'].mode()[0]\n",
    "    if print_results:\n",
    "        print('Real:',int(real_flower_type), ', Prediction:', predicted_flower_type)\n",
    "    \n",
    "    return predicted_flower_type == int(real_flower_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's use that function to test one of the meaures, the Euclidean distance:\n",
    "\n",
    "Feel free to change the K_NEAREST_NEIGHBOURS value. \n",
    "\n",
    "* What do you get if it is set to 120? Why do you think that is happening?\n",
    "* Can you find the best performing K_NEAREST_NEIGHBOURS value? What is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real: 1 , Prediction: 2\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 2 , Prediction: 1\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 1 , Prediction: 1\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 0 , Prediction: 0\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 2 , Prediction: 2\n",
      "Real: 2 , Prediction: 2\n",
      "Using Euclidean distance, we correctly predicted 73 / 75 test samples ( 97.33 %)\n"
     ]
    }
   ],
   "source": [
    "test_arrays = test.values.tolist()\n",
    "\n",
    "K_NEAREST_NEIGHBOURS = 5\n",
    "\n",
    "num_correct_predictions = 0\n",
    "\n",
    "for test_array in test_arrays:\n",
    "    correct_prediction = predict_using_k_nearest_neighbours(\n",
    "        test_array, \n",
    "        train, \n",
    "        euclidean_distance, \n",
    "        similarity=False, \n",
    "        k=K_NEAREST_NEIGHBOURS,\n",
    "        print_results=True\n",
    "    )\n",
    "    if correct_prediction:\n",
    "        num_correct_predictions += 1\n",
    "        \n",
    "print('Using Euclidean distance, we correctly predicted', num_correct_predictions,'/',len(test_arrays),\n",
    "     'test samples (', round(100*num_correct_predictions/len(test_arrays), 2), '%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can make a function from the above piece of code to test all of the functions:\n",
    "\n",
    "We will need to specify whether each meaure is a distance or a similarity \n",
    "(if they were normalised in the [0, 1] interval we won't need this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_using_knn_with_our_own_measure(test_dataframe, train_dataframe, measure, similarity, k=10):\n",
    "    test_arrays = test_dataframe.values.tolist()\n",
    "    num_correct_predictions = 0\n",
    "    \n",
    "    if similarity == 'similarity':\n",
    "        is_similarity = True # it is a similarity, the higher it is, the better\n",
    "    else:\n",
    "        is_similarity = False # it is a distance, the lower it is, the better\n",
    "\n",
    "    for test_array in test_arrays:\n",
    "        correct_prediction = predict_using_k_nearest_neighbours(\n",
    "            test_array, \n",
    "            train, \n",
    "            measure, \n",
    "            similarity=is_similarity, \n",
    "            k=k\n",
    "        )\n",
    "        if correct_prediction:\n",
    "            num_correct_predictions += 1\n",
    "\n",
    "    print('Using', measure.__name__, ', we correctly predicted', num_correct_predictions,'/',len(test_arrays),\n",
    "         'test samples (', round(100*num_correct_predictions/len(test_arrays), 2), '%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using euclidean_distance , we correctly predicted 72 / 75 test samples ( 96.0 %)\n",
      "Using manhattan_distance , we correctly predicted 72 / 75 test samples ( 96.0 %)\n",
      "Using cosine_similarity , we correctly predicted 72 / 75 test samples ( 96.0 %)\n",
      "Using pearson , we correctly predicted 70 / 75 test samples ( 93.33 %)\n",
      "Using spearman , we correctly predicted 52 / 75 test samples ( 69.33 %)\n",
      "Using discrete_similarity , we correctly predicted 23 / 75 test samples ( 30.67 %)\n",
      "Using hamming_distance , we correctly predicted 71 / 75 test samples ( 94.67 %)\n",
      "Using jaccard_similarity , we correctly predicted 71 / 75 test samples ( 94.67 %)\n"
     ]
    }
   ],
   "source": [
    "#Â MODIFY THIS CODE TO TEST YOUR OWN K_NEAREST_NEIGHBOURS PARAMETERS AND SEE HOW THAT CHANGES PERFORMANCES\n",
    "\n",
    "K_NEAREST_NEIGHBOURS = 1\n",
    "\n",
    "measures = (\n",
    "    (euclidean_distance, 'distance'),\n",
    "    (manhattan_distance, 'distance'),\n",
    "    (cosine_similarity, 'similarity'),\n",
    "    (pearson, 'similarity'),\n",
    "    (spearman, 'similarity'),\n",
    "    (discrete_similarity, 'similarity'),\n",
    "    (hamming_distance, 'distance'),\n",
    "    (jaccard_similarity, 'similarity'),\n",
    ")\n",
    "\n",
    "for measure, similarity in measures:\n",
    "    test_dataset_using_knn_with_our_own_measure(\n",
    "        test,\n",
    "        train,\n",
    "        measure,\n",
    "        similarity,\n",
    "        k=K_NEAREST_NEIGHBOURS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using combined_similarity , we correctly predicted 72 / 75 test samples ( 96.0 %)\n"
     ]
    }
   ],
   "source": [
    "# And why not defining our own similarity function based on other similarity functions? \n",
    "# As mentioned in the session, our imagination is the only limit.\n",
    "\n",
    "# After seeing that pearson and cosine were two of the best performing similarities, we\n",
    "# can use both of them and average them out... in some data permutations this produces \n",
    "# 100% accuracy. \n",
    "\n",
    "#Â MODIFY THIS CODE TO TEST YOUR OWN SIMILARITY FUNCTIONS IN THIS SIMULATION IF YOU WANT\n",
    "\n",
    "# See here how we transformed the Euclidean distance into Euclidean similarity as we\n",
    "# pointed out in the session slides.\n",
    "def combined_similarity(array1, array2):\n",
    "    total = (1/(1+euclidean_distance(array1, array2))) + cosine_similarity(array1, array2) + pearson(array1, array2)\n",
    "    return total/3\n",
    "\n",
    "test_dataset_using_knn_with_our_own_measure(\n",
    "        test,\n",
    "        train,\n",
    "        combined_similarity,\n",
    "        similarity='similarity',\n",
    "        k=K_NEAREST_NEIGHBOURS\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Exercises\n",
    "\n",
    "\n",
    "* If you decrease the size of the train dataset, how does it affect to the overall accuracy? (at the beginning, in the function `train_test_split(iris, test_size=0.5)`, what if test_size is just 0.05, or 0.95?)\n",
    "* Does using more nearest neighbours yield better results? Can you check by tuning the K_NEAREST_NEIGHBOURS parameter?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
