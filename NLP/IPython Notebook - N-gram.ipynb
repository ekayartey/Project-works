{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAyqKMsbkKBD",
        "outputId": "ed06aa8c-c319-4a14-9b38-d8e2c18107b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmwbdFSvkKBH"
      },
      "source": [
        "I adapted [this notebook](https://github.com/Elucidation/Ngram-Tutorial/blob/master/NgramTutorial.ipynb) to Python 3. \n",
        "\n",
        "## IPython Notebook - N-gram Tutorial\n",
        "\n",
        "Here is the explanation from the original author:\n",
        "\n",
        "_What we want to do is build up a dictionary of N-grams, which are pairs, triplets or more (the N) of words that pop up in the training data, with the value being the number of times they showed up. After we have this dictionary, as a naive example we could actually predict sentences by just randomly choosing words within this dictionary and doing a weighted random sample of the connected words that are part of n-grams within the keys._\n",
        "\n",
        "_Lets see how far we can get with N-grams without outside resources._\n",
        "\n",
        "_We have a text file for [Pride and Prejudice from Project Gutenberg](https://www.gutenberg.org/ebooks/1342) stored as pg1342.txt in the same folder as our notebook, but also available online directly. Let's load the text to a string since it's only 701KB, which will fit in memory nowadays._\n",
        "\n",
        "_**Note** : If we wanted to be more memory efficient we should parse the text file on a line or character by character basis, storing as needed, etc._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu50g-G2kKBJ",
        "outputId": "42695a2f-b631-4f21-8dea-1578d172d3d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "762943 , ï»¿The Project Gutenberg eBook of Pride and prejudic ...\n"
          ]
        }
      ],
      "source": [
        "# Find the number links by looking on Project Gutenberg in the address bar for a book.\n",
        "books = {'Pride and Prejudice': '1342',\n",
        "         'Huckleberry Fin': '76',\n",
        "         'Sherlock Holmes': '1661'}\n",
        "\n",
        "book = books['Pride and Prejudice']\n",
        "\n",
        "# Load text from Project Gutenberg URL\n",
        "import requests\n",
        "url_template = 'https://www.gutenberg.org/cache/epub/%s/pg%s.txt'\n",
        "\n",
        "response = requests.get(url_template % (book, book), 'r')\n",
        "txt = response.text\n",
        "\n",
        "# See the number of characters and the first 50 characters to confirm it is there    \n",
        "print(len(txt), ',', txt[:50] , '...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ3NsU_YkKBL"
      },
      "source": [
        "Great, now lets split into words into a big list, splitting on anything non-alphanumeric [A-Za-z0-9] (as well as punctuation) and forcing everything lowercase:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zN6o9o7HkKBL",
        "outputId": "596db04c-540d-4b98-e3ae-4201c216cb61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "131679\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "words = re.split('[^A-Za-z]+', txt.lower())\n",
        "words = list(filter(None, words)) # Remove empty strings\n",
        "\n",
        "# Print length of list\n",
        "print(len(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ9nqt5ZkKBN"
      },
      "source": [
        "## N-grams generation\n",
        "From this we can now generate N-grams, lets start with a 1-gram, basically the set of all the words\n",
        "\n",
        "**Note** : One could use a dictionary instead of a set and keeping count of the occurances gives word frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iM2UiQ6kKBO",
        "outputId": "c1040aca-b495-4400-bec7-0a1146d5fd3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6981\n",
            "['allayed', 'crisis', 'losses', 'finishing', 'polished', 'magnificent', 'suffered', 'letting', 'predict', 'follows', 'dependent', 'shortly', 'deal', 'unalterable', 'abusing', 'proficiency', 'deductible', 'moreover', 'judge', 'extracts']\n"
          ]
        }
      ],
      "source": [
        "# Create set of all unique words, this throws away any information about frequency however\n",
        "gram1 = set(words)\n",
        "\n",
        "print(len(gram1))\n",
        "\n",
        "# Print 20 elements of the set only\n",
        "print(list(gram1)[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NebDgI6JkKBP"
      },
      "source": [
        "Lets try and get the 2-gram now, which is pairs of words. Let's have a quick look to see the last 10 and how they look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqYMe0x6kKBP",
        "outputId": "1e10a7b6-c7f3-43e9-e31e-6de832ab796a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "subscribe to\n",
            "to our\n",
            "our email\n",
            "email newsletter\n",
            "newsletter to\n",
            "to hear\n",
            "hear about\n",
            "about new\n",
            "new ebooks\n"
          ]
        }
      ],
      "source": [
        "# See the last 10 pairs\n",
        "for i in range(len(words)-10, len(words)-1):\n",
        "    print(words[i], words[i+1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3ZWH3YxkKBQ"
      },
      "source": [
        "Okay, seems good, lets get all word pairs, and then generate a set of unique pairs from it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttZIatpxkKBR",
        "outputId": "72a4166b-0264-4a0e-f774-6ad479dc3c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "131678\n",
            "58228\n",
            "[('offered', 'olive'), ('himself', 'he'), ('man', 'often'), ('a', 'quantity'), ('nor', 'falsely'), ('to', 'contradict'), ('more', 'gentle'), ('lately', 'gone'), ('never', 'allow'), ('am', 'sure'), ('himself', 'her'), ('hardly', 'help'), ('way', 'his'), ('her', 'nose'), ('subject', 'drop'), ('wishing', 'to'), ('illustration', 'they'), ('unjustly', 'she'), ('perceiving', 'whom'), ('forget', 'all')]\n"
          ]
        }
      ],
      "source": [
        "word_pairs = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
        "print(len(word_pairs))\n",
        "\n",
        "gram2 = set(word_pairs)\n",
        "print(len(gram2))\n",
        "\n",
        "# Print 20 elements from gram2\n",
        "print(list(gram2)[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOxDP9EykKBR"
      },
      "source": [
        "## N-Grams Frequency\n",
        "Okay, that was fun, but this isn't enough, we need frequency if we want to have any sense of probabilities, which is what N-grams are about. Instead of using sets, lets create a dictionary with counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuzST1k8kKBS",
        "outputId": "082d4081-6544-4403-9d26-ad86e59603e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 4853), ('to', 4407), ('of', 3964), ('and', 3838), ('her', 2284), ('i', 2122), ('a', 2096), ('in', 2054), ('was', 1878), ('she', 1751), ('that', 1661), ('it', 1605), ('not', 1528), ('you', 1451), ('he', 1361), ('his', 1303), ('be', 1281), ('as', 1240), ('had', 1186), ('with', 1150)]\n"
          ]
        }
      ],
      "source": [
        "gram1 = dict()\n",
        "\n",
        "# Populate 1-gram dictionary\n",
        "for word in words:\n",
        "    if word in gram1:\n",
        "        gram1[word] += 1\n",
        "    else:\n",
        "        gram1[word] = 1 # Start a new entry with 1 count since saw it for the first time.\n",
        "\n",
        "# Turn into a list of (word, count) sorted by count from most to least\n",
        "gram1 = sorted(gram1.items(), key=lambda item: -item[1])\n",
        "\n",
        "# Print top 20 most frequent words\n",
        "print([(word, freq) for word, freq in gram1[:20]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luNqV8qTkKBT"
      },
      "source": [
        "For Pride and Prejudice, the words 'the', 'to', 'of', and 'and' were the top four most common words. Sounds about right, not too interesting yet, lets see what happens with 2-grams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQCEvESckKBT",
        "outputId": "7abf03c6-81c5-4f4c-c598-edc42d938a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('of', 'the'), 542), (('to', 'be'), 448), (('in', 'the'), 440), (('i', 'am'), 312), (('to', 'the'), 281), (('mr', 'darcy'), 277), (('of', 'her'), 275), (('it', 'was'), 255), (('of', 'his'), 242), (('she', 'was'), 213), (('it', 'is'), 210), (('had', 'been'), 206), (('she', 'had'), 206), (('i', 'have'), 191), (('to', 'her'), 186), (('that', 'he'), 181), (('and', 'the'), 174), (('could', 'not'), 172), (('for', 'the'), 170), (('he', 'had'), 167)]\n"
          ]
        }
      ],
      "source": [
        "gram2 = dict()\n",
        "\n",
        "# Populate 2-gram dictionary\n",
        "for i in range(len(words)-1):\n",
        "    key = (words[i], words[i+1])\n",
        "    if key in gram2:\n",
        "        gram2[key] += 1\n",
        "    else:\n",
        "        gram2[key] = 1\n",
        "\n",
        "# Turn into a list of (word, count) sorted by count from most to least\n",
        "gram2 = sorted(gram2.items(), key=lambda item: -item[1])\n",
        "\n",
        "# Print top 20 most frequent words\n",
        "print([(word, freq) for word, freq in gram2[:20]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49cvH9jZkKBU"
      },
      "source": [
        "It looks like \"of the\" and \"to be\" are the top two most common 2-grams, sounds good.\n",
        "\n",
        "##Â Next word prediction\n",
        "\n",
        "What can we do with this? Well lets see what happens if we take a random word from all the words, and build a sentence by just choosing the most common pair that has that word as it's start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUzspVv7kKBU",
        "outputId": "0f16785c-c103-4f70-d385-72a8b4ef041f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "him\n"
          ]
        }
      ],
      "source": [
        "start_word = words[int(len(words)/4)]\n",
        "print(start_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75FCVNFikKBV"
      },
      "source": [
        "I just went ahead and chose the word that appears $1/4$ of the way into words, random enough.\n",
        "\n",
        "Now in a loop, iterate through the frequency list (most frequent first) and see if it matches the first word in a pair, if so, the next word is the second element in the word pair, and continue with that word. Stop after N words or the list does not contain that word.\n",
        "\n",
        "**Note** : gram2 is a list that contains (key,value) where key is a word pair (first, second),\n",
        "           so you need element[0][0] for first word and element [0][1] for second word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0119PH9kKBV",
        "outputId": "0b2c208d-5729-489c-9f97-788d61d4fd67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start word: him\n",
            "2-gram sentence: \" him to be so much as to be so much as to be so much as to be so much \"\n"
          ]
        }
      ],
      "source": [
        "def get2GramSentence(word, n = 50):\n",
        "    words = []\n",
        "    for i in range(n):\n",
        "        words.append(word)\n",
        "        # Find Next word\n",
        "        word = next((element[0][1] for element in gram2 if element[0][0] == word), None)\n",
        "        if not word:\n",
        "            break\n",
        "    return ' '.join(words)\n",
        "\n",
        "word = start_word\n",
        "print(\"Start word:\", word)\n",
        "\n",
        "print('2-gram sentence: \"', get2GramSentence(word, 20), '\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnT2j3sikKBW"
      },
      "source": [
        "It gets stuck in a loop pretty much straight away. Not very interesting, try out other words and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx9kkbKlkKBX",
        "outputId": "87ef91e8-21b1-465f-ab7d-bcf1623169cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start word: and\n",
            "2-gram sentence: \" and the same time to be so much as to be so much as to be so much as to \"\n",
            "Start word: he\n",
            "2-gram sentence: \" he had been so much as to be so much as to be so much as to be so much \"\n",
            "Start word: she\n",
            "2-gram sentence: \" she was not be so much as to be so much as to be so much as to be so \"\n",
            "Start word: when\n",
            "2-gram sentence: \" when she was not be so much as to be so much as to be so much as to be \"\n",
            "Start word: john\n",
            "2-gram sentence: \" john thorpe the same time to be so much as to be so much as to be so much as \"\n",
            "Start word: never\n",
            "2-gram sentence: \" never be so much as to be so much as to be so much as to be so much as \"\n",
            "Start word: i\n",
            "2-gram sentence: \" i am sure i am sure i am sure i am sure i am sure i am sure i am \"\n",
            "Start word: how\n",
            "2-gram sentence: \" how much as to be so much as to be so much as to be so much as to be \"\n"
          ]
        }
      ],
      "source": [
        "for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
        "    print(\"Start word:\", word)\n",
        "    print('2-gram sentence: \"', get2GramSentence(word, 20), '\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgAJa1mXkKBX"
      },
      "source": [
        "## Weighted random choice based on frequency\n",
        "\n",
        "**This is our simple probabilistic MLE N-gram model**\n",
        "\n",
        "Same thing. Okay, lets randomly choose from the subset of all 2grams that matches the first word, using a weighted-probability based on counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kCxx9xiDkKBX"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def weighted_choice(choices):\n",
        "    total = sum(w for c, w in choices)\n",
        "    r = random.uniform(0, total)\n",
        "    upto = 0\n",
        "    for c, w in choices:\n",
        "        if upto + w > r:\n",
        "            return c\n",
        "        upto += w\n",
        "    \n",
        "def get2GramSentenceRandom(word, n = 50):\n",
        "    words = []\n",
        "    for i in range(n):\n",
        "        words.append(word)\n",
        "        # Get all possible elements ((first word, second word), frequency)\n",
        "        choices = [element for element in gram2 if element[0][0] == word]\n",
        "        if not choices:\n",
        "            break\n",
        "        \n",
        "        # Choose a pair with weighted probability from the choice list\n",
        "        word = weighted_choice(choices)[1]\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME5wETQmkKBY",
        "outputId": "66ce2ec7-a6d4-4323-8cff-27ebe195a5e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start word: and\n",
            "2-gram sentence: \" and reluctant good wishes it between the neighbourhood it to my side and elizabeth of her purchases were alone than \"\n",
            "Start word: he\n",
            "2-gram sentence: \" he believed sincere pleasure of fancy themselves needlessly long outdone by that it without its propriety in her companions a \"\n",
            "Start word: she\n",
            "2-gram sentence: \" she was the door was not to be frightened by asking whether her has lydia s two youngest i propose \"\n",
            "Start word: when\n",
            "2-gram sentence: \" when i have a respect towards conversing easily with regret invectives against such an affection for fortune though it to \"\n",
            "Start word: john\n",
            "2-gram sentence: \" john with his entrance hall they were to dinner was expected for and various claims on to have talked incessantly \"\n",
            "Start word: never\n",
            "2-gram sentence: \" never wanted to ask of atonement he is dead i should suffer from a smile of affection of saying he \"\n",
            "Start word: i\n",
            "2-gram sentence: \" i will only the man as all means you must be exposing him more lucky in the proposals even to \"\n",
            "Start word: how\n",
            "2-gram sentence: \" how i may seem to his resolving to posterity with any husband and at netherfield again and was sufficiently amused \"\n"
          ]
        }
      ],
      "source": [
        "for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
        "    print(\"Start word:\", word)\n",
        "    print('2-gram sentence: \"', get2GramSentenceRandom(word, 20), '\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0GBLjZ4kKBZ"
      },
      "source": [
        "Now that's way more interesting! Those are starting to look like sentences!\n",
        "\n",
        "Let's try a longer sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3VjXz8wkKBZ",
        "outputId": "2e070f7b-3398-4e78-8c8a-b36b6ed7d905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start word: it\n",
            "2-gram sentence: \" it give and blessing denied knowing it is the arts which she walked about the size of my sister elizabeth was of compliance for such spasms in a pleasant i that he will not one good opinion of preaching and had some others this effect of superior dancing at the \"\n"
          ]
        }
      ],
      "source": [
        "word = 'it'\n",
        "print(\"Start word:\", word)\n",
        "print('2-gram sentence: \"', get2GramSentenceRandom(word, 50), '\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdZUAUA5kKBZ"
      },
      "source": [
        "Pretty cool, lets see what happens when we go to N-grams above 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZmKLe_ukKBa"
      },
      "source": [
        "##Â Tri-grams and more\n",
        "Okay, let's create a Ngram generator that can let us make ngrams of arbitrary sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yr7HhdMIkKBa",
        "outputId": "6350fae0-f0df-4d65-c629-e98f94ed4167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('i', 'do', 'not'), 68), (('i', 'am', 'sure'), 64), (('project', 'gutenberg', 'tm'), 57), (('as', 'soon', 'as'), 56), (('she', 'could', 'not'), 53), (('that', 'he', 'had'), 37), (('in', 'the', 'world'), 36), (('copyright', 'by', 'george'), 35), (('by', 'george', 'allen'), 35), (('i', 'am', 'not'), 34), (('it', 'would', 'be'), 34), (('the', 'project', 'gutenberg'), 33), (('i', 'dare', 'say'), 30), (('it', 'was', 'not'), 30), (('that', 'he', 'was'), 30), (('mr', 'darcy', 's'), 30), (('as', 'well', 'as'), 29), (('could', 'not', 'be'), 29), (('would', 'have', 'been'), 28), (('that', 'it', 'was'), 28)]\n"
          ]
        }
      ],
      "source": [
        "def generateNgram(n=1):\n",
        "    gram = dict()\n",
        "    \n",
        "    # Some helpers to keep us crashing the PC for now\n",
        "    assert n > 0 and n < 100\n",
        "    \n",
        "    # Populate N-gram dictionary\n",
        "    for i in range(len(words)-(n-1)):\n",
        "        key = tuple(words[i:i+n])\n",
        "        if key in gram:\n",
        "            gram[key] += 1\n",
        "        else:\n",
        "            gram[key] = 1\n",
        "\n",
        "    # Turn into a list of (word, count) sorted by count from most to least\n",
        "    gram = sorted(gram.items(), key=lambda item: -item[1])\n",
        "    return gram\n",
        "\n",
        "trigram = generateNgram(3)\n",
        "# Print top 20 most frequent ngrams\n",
        "print(trigram[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhmquN3pkKBb",
        "outputId": "7a942660-d552-4540-a672-7ac2f97eab1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 2-gram list...\n",
            "Done\n",
            "Start word: and\n",
            "2-gram sentence: \n",
            "\"and mr bingley whose affection could she blushed and her sisters the short time to mr collins through the most\"\n",
            "\n",
            "Start word: he\n",
            "2-gram sentence: \n",
            "\"he replied darcy may wish his addressing him again when thus removed with only shook his mind was all about\"\n",
            "\n",
            "Start word: she\n",
            "2-gram sentence: \n",
            "\"she had been glad you but elizabeth for falling in spite of my feelings in any other way to her\"\n",
            "\n",
            "Start word: when\n",
            "2-gram sentence: \n",
            "\"when have no rest of their visitor was not forgot perhaps at the master soon after what she had not\"\n",
            "\n",
            "Start word: john\n",
            "2-gram sentence: \n",
            "\"john dashwood the evening between them but if your ladyship s regiment their aunt when the subject as they did\"\n",
            "\n",
            "Start word: never\n",
            "2-gram sentence: \n",
            "\"never saw much wretched suspense could not so daring to console lady catherine how ashamed of awkward as you to\"\n",
            "\n",
            "Start word: i\n",
            "2-gram sentence: \n",
            "\"i should communicate and prejudice far as possible the book mr collins s joints of the familiarity which she would\"\n",
            "\n",
            "Start word: how\n",
            "2-gram sentence: \n",
            "\"how mr bingley may contain a peculiar kind invitation to him when bingley but without having ever was sufficiently unlike\"\n",
            "\n",
            "***************************************************************************\n",
            "Generating 3-gram list...\n",
            "Done\n",
            "Start word: and\n",
            "3-gram sentence: \n",
            "\"and watch for those beautiful for almost resolving to provide a warmth remained quietly in touches of sentiment was forced\"\n",
            "\n",
            "Start word: he\n",
            "3-gram sentence: \n",
            "\"he thought much apprehension of their indignation that they were post and the room for in a more than myself\"\n",
            "\n",
            "Start word: she\n",
            "3-gram sentence: \n",
            "\"she made no occasion for there is a flirt with regard for its ever whilst you happy woman in accepting\"\n",
            "\n",
            "Start word: when\n",
            "3-gram sentence: \n",
            "\"when i must just been impossible for the avowal of them the mortifying conviction that kind of a mile and\"\n",
            "\n",
            "Start word: john\n",
            "3-gram sentence: \n",
            "\"john dashwood the course removed after herself most improbable kitty who was it is luckily too well bred and they\"\n",
            "\n",
            "Start word: never\n",
            "3-gram sentence: \n",
            "\"never have mr darcy likely to the deception never without the following morning to know said elizabeth i confess said\"\n",
            "\n",
            "Start word: i\n",
            "3-gram sentence: \n",
            "\"i have a word from his behaviour to have determined at netherfield party and from this i do not do\"\n",
            "\n",
            "Start word: how\n",
            "3-gram sentence: \n",
            "\"how everybody is nothing of success she could forgive all his last met for i hope of his sisters mr\"\n",
            "\n",
            "***************************************************************************\n",
            "Generating 4-gram list...\n",
            "Done\n",
            "Start word: and\n",
            "4-gram sentence: \n",
            "\"and their cousin s opinion of marriage she could observe my poor with family at the collection despite the matter\"\n",
            "\n",
            "Start word: he\n",
            "4-gram sentence: \n",
            "\"he congratulated both sides but my love to ask you have two or wished to yield readily you of any\"\n",
            "\n",
            "Start word: she\n",
            "4-gram sentence: \n",
            "\"she did not exactly what would all her character though what do extremely glad to leave to her again i\"\n",
            "\n",
            "Start word: when\n",
            "4-gram sentence: \n",
            "\"when they had been highly advisable for her observation would now a settled at longbourn was last man really liked\"\n",
            "\n",
            "Start word: john\n",
            "4-gram sentence: \n",
            "\"john told by bingley had it you should be the compliment before downloading copying distributing any self importance and repulsive\"\n",
            "\n",
            "Start word: never\n",
            "4-gram sentence: \n",
            "\"never gave me only entailed in the expression of almost great advantage to make some kind of wednesday and the\"\n",
            "\n",
            "Start word: i\n",
            "4-gram sentence: \n",
            "\"i dare pronounce herself so want this electronic work associated with whatever his observation that he expressed her friend not\"\n",
            "\n",
            "Start word: how\n",
            "4-gram sentence: \n",
            "\"how to be done and vain a life i have no hesitation in the luckiest family knew that will persuade\"\n",
            "\n",
            "***************************************************************************\n",
            "Generating 5-gram list...\n",
            "Done\n",
            "Start word: and\n",
            "5-gram sentence: \n",
            "\"and said she spoke wickham has abilities which mr collins there was not be placed himself particularly welcome intelligence of\"\n",
            "\n",
            "Start word: he\n",
            "5-gram sentence: \n",
            "\"he wishes it to read it was destined by night tell you have him thither almost entirely but they must\"\n",
            "\n",
            "Start word: she\n",
            "5-gram sentence: \n",
            "\"she had brought him at the critical eye on his side and lady catherine was watching how i am always\"\n",
            "\n",
            "Start word: when\n",
            "5-gram sentence: \n",
            "\"when they continued the dimensions of his constancy if her pleasure for so well of that is indeed yes replied\"\n",
            "\n",
            "Start word: john\n",
            "5-gram sentence: \n",
            "\"john told her sister s objections to hope you discourage his aunt did at his breakfast was the affair they\"\n",
            "\n",
            "Start word: never\n",
            "5-gram sentence: \n",
            "\"never worse bred than ten as well lizzy when her but she then rallied his friend and he might have\"\n",
            "\n",
            "Start word: i\n",
            "5-gram sentence: \n",
            "\"i was most accomplished nor in fault with some degree in meditating a few of assisting mrs gardiner highly gratified\"\n",
            "\n",
            "Start word: how\n",
            "5-gram sentence: \n",
            "\"how wonderfully clever in town three miles in paragraph f f indemnity you would not be on his expressions which\"\n",
            "\n",
            "***************************************************************************\n",
            "Generating 6-gram list...\n",
            "Done\n",
            "Start word: and\n",
            "6-gram sentence: \n",
            "\"and could do all my heart on my children produced but he could not gone the means of masters my\"\n",
            "\n",
            "Start word: he\n",
            "6-gram sentence: \n",
            "\"he has somewhere i was herself and more than she turned her sentiments avowed to take it so far from\"\n",
            "\n",
            "Start word: she\n",
            "6-gram sentence: \n",
            "\"she was impossible that miss the man often told him credit of these pleasing said colonel forster to profit c\"\n",
            "\n",
            "Start word: when\n",
            "6-gram sentence: \n",
            "\"when mrs bennet gone and spirit and it inevitable and my nephew it falls in the lane london once sit\"\n",
            "\n",
            "Start word: john\n",
            "6-gram sentence: \n",
            "\"john told him here soon as she had feared to receive them whenever he might be psychologically speaking i am\"\n",
            "\n",
            "Start word: never\n",
            "6-gram sentence: \n",
            "\"never speaks much recovered but he was in a little mr collins moreover caught and backed by caroline in love\"\n",
            "\n",
            "Start word: i\n",
            "6-gram sentence: \n",
            "\"i have nothing else but i hope we darcy had i assure you hold her own interest has related the\"\n",
            "\n",
            "Start word: how\n",
            "6-gram sentence: \n",
            "\"how should not the superior to that they intended for me to think i said elizabeth listened with laughing and\"\n",
            "\n",
            "***************************************************************************\n",
            "Generating 7-gram list...\n",
            "Done\n",
            "Start word: and\n",
            "7-gram sentence: \n",
            "\"and in his feeling of her she assured her with me by such very sweet animation and writing from him\"\n",
            "\n",
            "Start word: he\n",
            "7-gram sentence: \n",
            "\"he has been most earnestly looking at lambton with lady in hertfordshire may live upon elizabeth when we were really\"\n",
            "\n",
            "Start word: she\n",
            "7-gram sentence: \n",
            "\"she felt depressed before you saw mr darcy you equal sincerity said lydia illustration to introduce yourself into the best\"\n",
            "\n",
            "Start word: when\n",
            "7-gram sentence: \n",
            "\"when miss bennet s happiness and prejudice her he does not suppose there was worse and so desirous of her\"\n",
            "\n",
            "Start word: john\n",
            "7-gram sentence: \n",
            "\"john told you i get him at longbourn estate there was full of the time that he had generally is\"\n",
            "\n",
            "Start word: never\n",
            "7-gram sentence: \n",
            "\"never be a young ladies copyright by the idea she added my children and dazzling with her his side i\"\n",
            "\n",
            "Start word: i\n",
            "7-gram sentence: \n",
            "\"i have at the other words may be so easy temper to such a good one of a few short\"\n",
            "\n",
            "Start word: how\n",
            "7-gram sentence: \n",
            "\"how she declared to find that we must not be as she was never ridicule such extraordinary deference the family\"\n",
            "\n",
            "***************************************************************************\n",
            "Generating 8-gram list...\n",
            "Done\n",
            "Start word: and\n",
            "8-gram sentence: \n",
            "\"and mary wollstonecraft protested with the party true and complaints and general charlotte s purpose f gave her sister would\"\n",
            "\n",
            "Start word: he\n",
            "8-gram sentence: \n",
            "\"he acted there are precisely what i have alarmed but he turning towards him again at rosings park is ill\"\n",
            "\n",
            "Start word: she\n",
            "8-gram sentence: \n",
            "\"she retired to her nature is mr bingley s using or any alarm his gratitude towards mr bennet would not\"\n",
            "\n",
            "Start word: when\n",
            "8-gram sentence: \n",
            "\"when the dishes at longbourn but this quiet at heart on her spirits amidst your merit than by u s\"\n",
            "\n",
            "Start word: john\n",
            "8-gram sentence: \n",
            "\"john with the convenience of the feelings i shall not make it must marry to a distance and was at\"\n",
            "\n",
            "Start word: never\n",
            "8-gram sentence: \n",
            "\"never acknowledged favourite haunt of construction which unfortunately of doors to be renewed after him was very much surprised to\"\n",
            "\n",
            "Start word: i\n",
            "8-gram sentence: \n",
            "\"i was occasionally laughing alarm and though this time that mr bennet you she is better no cost and his\"\n",
            "\n",
            "Start word: how\n",
            "8-gram sentence: \n",
            "\"how it amusing study of a short and turning away the officers may be sensible a call him how lydia\"\n",
            "\n",
            "***************************************************************************\n",
            "Generating 9-gram list...\n",
            "Done\n",
            "Start word: and\n",
            "9-gram sentence: \n",
            "\"and return home to sacrifice to his information about by the room there are to you will do not propitious\"\n",
            "\n",
            "Start word: he\n",
            "9-gram sentence: \n",
            "\"he had advanced was affectionate mother talked of their employments and from her earnest of only by four to his\"\n",
            "\n",
            "Start word: she\n",
            "9-gram sentence: \n",
            "\"she remembered also and very good however had not escape from right by george allen had likewise extremely well yesterday\"\n",
            "\n",
            "Start word: when\n",
            "9-gram sentence: \n",
            "\"when miss bingley will not do and to chapter xxviii illustration cheerful and prejudice i want very minutely to be\"\n",
            "\n",
            "Start word: john\n",
            "9-gram sentence: \n",
            "\"john dashwood the indulgence the ball room and twenty my brother a complete propriety requested to be from all astonishment\"\n",
            "\n",
            "Start word: never\n",
            "9-gram sentence: \n",
            "\"never do not leave kent than that miss bennet was not located in the two sisters home mrs long before\"\n",
            "\n",
            "Start word: i\n",
            "9-gram sentence: \n",
            "\"i knew not this discovery succeeded no answer and as good of the necessary in the wisest and she really\"\n",
            "\n",
            "Start word: how\n",
            "9-gram sentence: \n",
            "\"how am quite northward than usual about it is a manner the mantel piece of her alone and now in\"\n",
            "\n",
            "***************************************************************************\n"
          ]
        }
      ],
      "source": [
        "def getNGramSentenceRandom(gram, word, n = 50):\n",
        "    words = []\n",
        "    for i in range(n):\n",
        "        words.append(word)\n",
        "        # Get all possible elements ((first word, second word), frequency)\n",
        "        choices = [element for element in gram if element[0][0] == word]\n",
        "        if not choices:\n",
        "            break\n",
        "        \n",
        "        # Choose a pair with weighted probability from the choice list\n",
        "        word = weighted_choice(choices)[1]\n",
        "    return ' '.join(words)\n",
        "\n",
        "for n in range(2,10):\n",
        "    # Generate ngram list\n",
        "    print(f\"Generating {n}-gram list...\")\n",
        "    ngram = generateNgram(n)\n",
        "    print(\"Done\")\n",
        "    \n",
        "    # Try out a bunch of sentences\n",
        "    for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
        "        print(\"Start word:\", word)\n",
        "        print(f'{n}-gram sentence: \\n\"{getNGramSentenceRandom(ngram, word, 20)}\"')\n",
        "        print()\n",
        "        \n",
        "    print('***************************************************************************')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBbCVXxXkKBc"
      },
      "source": [
        "The sentences produced by higher-level N-gram looks almost like normal sentences if you squint a little!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}