{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and other Dimensionality Reduction Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel-PCA Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans, DBSCAN\n",
    "from sklearn.datasets import make_circles, load_wine\n",
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS PARAMETERS TO CREATE OTHER CIRCLE-SHAPED DATASETS:\n",
    "input_features, _ = make_circles(\n",
    "    n_samples = 1000,\n",
    "    factor=0.25,\n",
    "    noise=0.1\n",
    ")\n",
    "df = pd.DataFrame(input_features)\n",
    "df.columns = ['x1', 'x2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['x1'], df['x2'])\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Original Data Spread')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is not linearly separable. Using K-means does not produce derisable clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = MiniBatchKMeans(\n",
    "    n_clusters=2,\n",
    "    init='k-means++',\n",
    ")\n",
    "kmeans_clusters = kmeans.fit_predict(df[['x1', 'x2']])\n",
    "\n",
    "df['kmeans'] = kmeans_clusters\n",
    "\n",
    "clus1 = df[df['kmeans'] == 0]\n",
    "clus2 = df[df['kmeans'] == 1]\n",
    "\n",
    "plt.scatter(clus1['x1'], clus1['x2'], color='b')\n",
    "plt.scatter(clus2['x1'], clus2['x2'], color='r')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Original Data Spread')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************************************************************************\n",
    "# I had to tweak this for a while until I got the right value of the eps distance!!!\n",
    "# This is not practical in some situations...\n",
    "# ************************************************************************************\n",
    "dbscan = DBSCAN(\n",
    "    eps=0.15,  \n",
    "    min_samples=5\n",
    ")\n",
    "dbscan_clusters = dbscan.fit_predict(df[['x1', 'x2']])\n",
    "df['dbscan'] = dbscan_clusters\n",
    "\n",
    "clus1 = df[df['dbscan'] == 0]\n",
    "clus2 = df[df['dbscan'] == 1]\n",
    "\n",
    "plt.scatter(clus1['x1'], clus1['x2'], color='b')\n",
    "plt.scatter(clus2['x1'], clus2['x2'], color='r')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Original Data Spread')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use KernelPCA with an RBF kernel to linearly separate these two clusters in a higher-dimensional space with a Kernel Trick!\n",
    "\n",
    "**In this case we are not using Kernel-PCA to reduce dimensionality**, we are just using it to project the data in a different 2D-space where it is linearly separated (because the original circles were also separated in a 2D plane, just they are not linearly separated!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLAY WITH THE INPUT PARAMETERS TO SEE HOW THIS WORKS:\n",
    "kpca = KernelPCA(\n",
    "    n_components = 2,\n",
    "    kernel='rbf',\n",
    "    fit_inverse_transform=True,\n",
    "    gamma=10,\n",
    "    random_state=1000\n",
    ")\n",
    "transformed_data = kpca.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pd.DataFrame(transformed_data)\n",
    "components.columns = ['pc1', 'pc2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(components['pc1'], components['pc2'])\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Principal Components Spread')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those clusters are clearly separated now, and in a linear way! Now even the simple K-Means algorithm will recognise the two classes immediately. Let's reuse the exact same model as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_clusters = kmeans.fit_predict(components[['pc1', 'pc2']])\n",
    "\n",
    "components['kmeans_pca'] = kmeans_clusters\n",
    "\n",
    "clus1 = components[components['kmeans_pca'] == 0]\n",
    "clus2 = components[components['kmeans_pca'] == 1]\n",
    "\n",
    "plt.scatter(clus1['pc1'], clus1['pc2'], color='b')\n",
    "plt.scatter(clus2['pc1'], clus2['pc2'], color='r')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Principal Components Spread with K-means applied:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not perfect but a much better approximation than the initial K-means. Let's plot these clusters in the original data now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kmeans_pca'] = components['kmeans_pca']\n",
    "\n",
    "clus1 = df[df['kmeans_pca'] == 0]\n",
    "clus2 = df[df['kmeans_pca'] == 1]\n",
    "\n",
    "plt.scatter(clus1['x1'], clus1['x2'], color='b')\n",
    "plt.scatter(clus2['x1'], clus2['x2'], color='r')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Original Data Spread')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA without num. components example\n",
    "\n",
    "In this example we will see how to create a PCA model that automatically selects the number of components.\n",
    "\n",
    "**Note:** There are a number of ways in which the `n_components` parameter can be instantiated:\n",
    "* If it is an integer over 1, it will be the number of components as we already saw.\n",
    "* If it is a value between 0 and 1 and the `svd_solver` is `full`, then the `n_components` parameter refers to the % of variance we want to maintain in the data, and the actual number of components will be selected via the `svd_solver`, meeting the variance condition.\n",
    "* If its value is `mle`, and `svd_solver` is `full`, then it will try to figure out everything based on the MLE algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df = load_wine(return_X_y=True, as_frame=True)\n",
    "wine_df = wine_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: Scale/Normalise your data!\n",
    "wine_df = StandardScaler(with_mean=0, with_std=1).fit_transform(wine_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have no idea of how many components I should set... let's do 10...\n",
    "vanilla_pca = PCA(n_components=10, svd_solver='auto')\n",
    "\n",
    "v_pca = vanilla_pca.fit_transform(wine_df)\n",
    "print('Shape of the PCA-transformed data: ',v_pca.shape)\n",
    "print('Those are', v_pca.shape[1], 'components')\n",
    "print('Variance explained: ', round(100*sum(vanilla_pca.explained_variance_ratio_), 2), '% of the total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what selection the MLE (Maximum Likelihood Estimator) comes up with:\n",
    "mle_pca = PCA(n_components='mle', svd_solver='full')\n",
    "\n",
    "mle_pca_data = mle_pca.fit_transform(wine_df)\n",
    "print('Shape of the PCA-transformed data: ',mle_pca_data.shape)\n",
    "print('Those are', mle_pca_data.shape[1], 'components')\n",
    "print('Variance explained: ', round(100*sum(mle_pca.explained_variance_ratio_), 2), '% of the total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many components we get if we are happy to maintain 65% of the variance:\n",
    "# FEEL FREE TO CHANGE THAT PARAMETER TO SEE HOW MANY COMPONENTS ARE LEFT\n",
    "var_pca = PCA(n_components=0.65, svd_solver='full')\n",
    "\n",
    "var_pca_data = var_pca.fit_transform(wine_df)\n",
    "print('Shape of the PCA-transformed data: ',var_pca_data.shape)\n",
    "print('Those are', var_pca_data.shape[1], 'components')\n",
    "print('Variance explained: ', round(100*sum(var_pca.explained_variance_ratio_), 2), '% of the total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(var_pca_data, columns=['PC1', 'PC2', 'PC3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just 3 components we can maintain over 65% of the variance of the original data!\n",
    "\n",
    "Since they are just 3 components, we can plot it in a 3D chart.\n",
    "\n",
    "**For some reason, I cannot visualise 3D charts in Jupyter Lab, only Jupyter Notebook**\n",
    "\n",
    "Note that in the chart the X-axis corresponds to the first Principal Component, and therefore the most important one, then the Y-axis and the Z-axis.\n",
    "\n",
    "If you explore the below chart a little bit, you might understand that there seems to be like 3 clusters: those could be the three classes of our original data*. Maybe? Let's run K-means with K=3 on it and see.\n",
    "\n",
    "*Yes, one thing we know already is that this dataset has 3 classes... In a real-world scenario we wouldn't know how many classes we have in reality. However lowering the dimension can help us visualise it. In this case, it seems like the 3D chart shows 3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(result, x='PC1', y='PC2', z='PC3',\n",
    "                    color_continuous_scale='Rainbow')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = MiniBatchKMeans(\n",
    "    n_clusters=3,\n",
    "    init='k-means++',\n",
    ")\n",
    "kmeans_clusters = kmeans.fit_predict(result[['PC1', 'PC2', 'PC3']])\n",
    "result['kmeans'] = kmeans_clusters\n",
    "\n",
    "fig = px.scatter_3d(result, x='PC1', y='PC2', z='PC3',\n",
    "                    color='kmeans', color_continuous_scale='Rainbow')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! That 3D chart looks good seems like. The 3 clusters are more or less well-defined.\n",
    "\n",
    "# Questions and learning exercises:\n",
    "\n",
    "Have we been able to discover the 3 original classes of the dataset using Dimensionality Reduction without using the original target feature? What do you think? Can you prove it? (using the original target feature of the dataset or otherwise)\n",
    "\n",
    "Can you achieve the same clustering result with DBSCAN, OPTICS or SpectralClustering? What about Gaussian Mixture Models? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
