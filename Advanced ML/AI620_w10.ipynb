{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Taxi Problem\n",
    "\n",
    "This Notebook is mostly based on the one [from here](https://casey-barr.github.io/open-ai-taxi-problem/), so refer to that link for more information! However I will, as usual, give you here as much detail as possible.\n",
    "\n",
    "\n",
    "### Problem Description:\n",
    "The Taxi Problem\n",
    "    from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\"\n",
    "    by Tom Dietterich\n",
    "    \n",
    "**Description:**\n",
    "  \n",
    "There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location (in the renders, this is the passenger location highglighted in **bold**). The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations, **coloured**), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "\n",
    "\n",
    "**Observations:** \n",
    "\n",
    "There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations. \n",
    "    \n",
    "**Passenger locations:**\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "    - 4: in taxi\n",
    "    \n",
    "**Destinations:**\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "    \n",
    "**Actions** (_There are 6 discrete deterministic actions_):\n",
    "    - 0: move south\n",
    "    - 1: move north\n",
    "    - 2: move east\n",
    "    - 3: move west\n",
    "    - 4: pickup passenger\n",
    "    - 5: drop off passenger\n",
    "    \n",
    "**Rewards:**\n",
    "There is a default per-step reward of -1, except for delivering the passenger, which is +20, or executing \"pickup\" and \"drop-off\" actions illegally, which is -10 (Counted as \"penalty\" in the code below).\n",
    "    \n",
    "**Rendering:**\n",
    "    - blue: passenger\n",
    "    - magenta: destination\n",
    "    - yellow: empty taxi\n",
    "    - green: full taxi\n",
    "    - other letters (R, G, Y and B): locations for passengers and destinations\n",
    "    state space is represented by:\n",
    "        (taxi_row, taxi_col, passenger_location, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from time import sleep\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\").env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset environment to a new, random state\n",
    "env.reset() \n",
    "\n",
    "# Renders the environment at the current status:\n",
    "# * Taxi location (yellow square), \n",
    "# * Passenger pick-up location as a bold and coloured letter, and \n",
    "# * Passenger drop-off location, as a coloured letter (not bold)\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve enviroment without reinforcement\n",
    "\n",
    "At every state, we just take a sample action... (not really great, is it?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility function to print the animations of all the actions taken by the taxi agent\n",
    "def print_frames(frames, sleep_time=0.1):\n",
    "    for i, frame in enumerate(frames):\n",
    "        sleep(sleep_time)\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1} / {len(frames)}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANIMATION_LENGTH = 30 # seconds\n",
    "        \n",
    "print_frames(frames, sleep_time=ANIMATION_LENGTH/len(frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Q-learning to train via reinforcement\n",
    "\n",
    "We will learn this algorithm later on in this course, but for now just bear in mind that it's based on a Markov Decision Process! (as well as Dynamic Programming and some Monte-Carlo ideas).\n",
    "\n",
    "You can see this Q-learning implementation as the last stage of the course. After we have seen all of the above in detail, you can come back to this Notebook and you will better understand the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "# Hyperparameters of the Q-learning algorithm\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "# Doing 100 runs of the Taxi problem, and averaging the results...\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our MDP-based Reinforced Learning results!\n",
    "\n",
    "Every time you run the cell below, a new random start point (state) is created, and then solved using the result of the Q-learning algorithm above: Now the actions our agent take are quite well informed! (**the policy is optimised!**) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "frames = []\n",
    "done=False\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "    \n",
    "ANIMATION_LENGTH = 10 # seconds\n",
    "\n",
    "print_frames(frames, sleep_time=ANIMATION_LENGTH/len(frames))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
