{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gym\n",
    "# You might need to re-start the Notebook after these installs:\n",
    "# !pip3 install pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To make this work:** \n",
    "\n",
    "Install the Gym library from OpenAI from the following URL http://gym.openai.com/ (or pip3 install gym and then manually add some dependencies - in my case pip install pyglet - also other stuff as this [X server for Windows](https://sourceforge.net/projects/vcxsrv/) - basically to allow Jupyter to open a window in your OS and visualise the 2D game of the Reinforced Learning environment. \n",
    "\n",
    "Gym is an interactive learning game toolkit for developing and comparing reinforcement learning algorithms. The environment is provided and you have to provide the algorithm. You can write your agent using your existing numerical computation library.\n",
    "\n",
    "https://kyso.io/eoin/openai-gym-jupyter\n",
    "\n",
    "\n",
    "\n",
    "# Reinforced Learning Hello World\n",
    "\n",
    "This week we are going to use a very simple example of Reinforced Learning: A cart (a brown box) which is trying to keep a stick standing vertically in balance. If the stick is falling to the right, the cart should compensate the fall and try to make the stick straight again.\n",
    "\n",
    "This week's interactive activity contains just two code cells:\n",
    "* In the first one, the agent (cart) is taking random actions\n",
    "* In the second one, the agent is taking informed actions (towards the goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for i in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "    if(i%100 == 0):\n",
    "        print(i)\n",
    "env.close()\n",
    "print('Simulation finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        # You can print the observations (status of the environment), \n",
    "        # which are numerical coordinates of the position of the cart and of the top of the stick\n",
    "#         print(observation)\n",
    "        # Then, an action is calculated\n",
    "        action = env.action_space.sample()\n",
    "        # By calling the next step of the environment, with the action taken,\n",
    "        # we will get the new observed parameters, the updated reward, and two more params related to gym\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        # One param is the \"done\" flag. When the reward is optimal (stick in balance), an \"episode\" is finished:\n",
    "        if done:\n",
    "            print(\"Episode finished after {} actions\".format(t+1))\n",
    "            break\n",
    "        # But the stick (because of momentum and gravity), will soon start falling again...\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
