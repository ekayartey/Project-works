{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced ensembles\n",
    "\n",
    "This week we are going to review more advanced ensembles in Sklearn and... XGBoost! \n",
    "\n",
    "We will focus on the following algorithms: \n",
    "\n",
    "* AdaBoost\n",
    "* GradientBoost (Sklearn)\n",
    "* XGBoost (XGBoost library)\n",
    "\n",
    "Our example data this week will be the Local Weather Archive in the Town of Cary, North Carolina. The dataset contains several input features about weather parameters, which we can use to predict whether it rained or not during a given week (binary classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by installing our little baby:\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And importing some stuff from SKlearn to run too:\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "import io\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now as usual, we will pick the open-data from a URL and create our Pandas DataFrame:\n",
    "data_url = 'https://data.townofcary.org/api/v2/catalog/datasets/rdu-weather-history/exports/csv'\n",
    "\n",
    "data_stream=requests.get(data_url).content\n",
    "df=pd.read_csv(io.StringIO(data_stream.decode('utf-8')), sep=';', engine='python')\n",
    "print('Rows in data:', len(df))\n",
    "print('Columns in data:', len(df.columns))\n",
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_delete = [\n",
    "    'fastest5secwinddir',\n",
    "    'fastest5secwindspeed', # always null\n",
    "    'date', # we won't use date\n",
    "    'precipitation', # we are going to classify a binary column called \"Rain\", if precipitation>0, then Rain is True always...\n",
    "    'drizzle', # if it drizzled, it rained...\n",
    "    'snow', # if it snowed, it rained too..\n",
    "    'blowingsnow',\n",
    "    'hail', # and same with hail\n",
    "]\n",
    "\n",
    "df.drop(columns_to_delete, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns from 7 onwards are all boolean:\n",
    "boolean_columns = df.columns[7:]\n",
    "print(list(boolean_columns))\n",
    "\n",
    "def make_dummy(value):\n",
    "    if(value == 'Present'):\n",
    "        return str(1)\n",
    "    else:\n",
    "        return str(0)\n",
    "    \n",
    "for b in boolean_columns:\n",
    "    df[b] = df[b].apply(make_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am just putting the 'rain' feature at the end of the DataFrame, nothing else (I like to have the target at the end):\n",
    "\n",
    "# IF YOU WANT TO PREDICT SOMETHING ELSE LIKE: mist, fog, fogheavy, ice, ETC. JUST CHANGE THIS!\n",
    "target_feature_name = 'rain'\n",
    "input_features = [c for c in df.columns if c != target_feature_name]\n",
    "\n",
    "target_feature = df[target_feature_name]\n",
    "df = df[input_features]\n",
    "df['rain'] = target_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, here we will impute the NaN values (not-a-number), using a KNN imputer: Setting the values of the NaN data points\n",
    "# as the average of their 5 nearest neighours.\n",
    "\n",
    "imputer = KNNImputer(missing_values=np.NaN, n_neighbors=5, weights='distance')\n",
    "df[input_features] = imputer.fit_transform(df[input_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, data is ready to rock!!\n",
    "\n",
    "#### Let's start with Sklearn:\n",
    "\n",
    "RandomForest, Bagging, AdaBoost and GradientBoost :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ESTIMATORS = 50 # 50 classifiers in each ensemble. FEEL FREE TO CHANGE THIS!\n",
    "\n",
    "X = df[input_features]\n",
    "y = df[target_feature_name].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to measure several parameters this time, not just accuracy. YOU CAN ADD OR REMOVE AS YOU LIKE\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "def print_results(scores):\n",
    "    for key in scores:\n",
    "        if key.startswith('test_'):\n",
    "            print(f'{key}:\\t{round(scores[key].mean(), 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baggingDT = BaggingClassifier(\n",
    "    base_estimator=None, # When this is NULL, Bagging will use DecisionTrees as the base estimator\n",
    "    n_estimators=NUM_ESTIMATORS,\n",
    "    max_samples=0.30,    # A maximum of 30% of the data will be used in each DecisionTree, FEEL FREE TO CHANGE THIS\n",
    "    n_jobs=-1,\n",
    ")\n",
    "print('Doing, ', baggingDT)\n",
    "bagging_score = cross_validate(\n",
    "    baggingDT, X, y, cv=5, scoring=metrics\n",
    ")\n",
    "print(baggingDT, 'done')\n",
    "print_results(bagging_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest = RandomForestClassifier(\n",
    "    n_estimators=NUM_ESTIMATORS,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "print('Doing, ', randomForest)\n",
    "rf_score = cross_validate(\n",
    "    randomForest, X, y, cv=5, scoring=metrics\n",
    ")\n",
    "print(randomForest, 'done')\n",
    "print_results(rf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaBoost = AdaBoostClassifier(\n",
    "    n_estimators=NUM_ESTIMATORS,\n",
    "    learning_rate=1,      # FEEL FREE TO CHANGE THIS TO TEST FOR BETTER RESULTS\n",
    "    algorithm='SAMME',    # SAMME works well for discrete outputs (SAMME.R is for regression remember). We don't have M1 for binary in Sklearn unfortunately\n",
    "    # n_jobs=-1, --> Because of the way it works (chaining/cascading classifiers), it cannot be parallellised!!\n",
    ")\n",
    "print('Doing, ', adaBoost)\n",
    "ab_score = cross_validate(\n",
    "    adaBoost, X, y, cv=5, scoring=metrics\n",
    ")\n",
    "print(adaBoost, 'done')\n",
    "print_results(ab_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradBoost = GradientBoostingClassifier(\n",
    "    n_estimators=NUM_ESTIMATORS,\n",
    "    learning_rate=0.1,    # FEEL FREE TO CHANGE THIS TO TEST FOR BETTER RESULTS\n",
    "    subsample=1,        # Each new classifier will be trained on 100% of the data (no bagging). If it's less than 1 (100%), we will be doing a STOCHASTIC gradient boosting\n",
    "    # n_jobs=-1, --> Because of the way it works (chaining/cascading classifiers), it cannot be parallellised!!\n",
    ")\n",
    "print('Doing, ', gradBoost)\n",
    "gb_score = cross_validate(\n",
    "    gradBoost, X, y, cv=5, scoring=metrics\n",
    ")\n",
    "print(gradBoost, 'done')\n",
    "print_results(gb_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_overview = {\n",
    "    'bagging': bagging_score,\n",
    "    'randomforest': rf_score,\n",
    "    'adaboost': ab_score,\n",
    "    'gradient_boost': gb_score,\n",
    "}\n",
    "\n",
    "scores_data = []\n",
    "for algo in score_overview:\n",
    "    for metric in score_overview[algo]:\n",
    "        row = [algo]\n",
    "        row.append(metric)\n",
    "        row.append(score_overview[algo][metric].mean())\n",
    "        scores_data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(scores_data)\n",
    "scores.columns = ['algorithm', 'metric', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15,7)\n",
    "sns.barplot(data=scores, x='metric', y='score', hue='algorithm', saturation=0.6, palette='Blues',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary:\n",
    "(As the results were when I ran this Notebook - you might get slightly different ones as cross-validation is shuffling the data) \n",
    "\n",
    "**fit_time** and **score_time**\n",
    "While AdaBoost and GradientBoost are slower when fitting the training data, they are much faster when making predictions (once the model is trained, to output a class for new input data). Particularly GradientBoost is just a small fraction of the rest. So once the model is trained, it will be much faster in a production environment.\n",
    "\n",
    "**Accuracy**\n",
    "Accuracies are quite similar for all 4 ensemble classifiers - however GradientBoost peaks a little bit over the rest.\n",
    "\n",
    "**Precision** and **Recall**\n",
    "Again more or less the same in all of them. AdaBoost appears to have sacrificied a little bit of precision to maximise recall in this instance. Bagging and RandomForest are the opposite than AdaBoost. GradientBoost seems to be balancing the two.\n",
    "\n",
    "**F1-score** and **ROC AUC**\n",
    "AdaBoost and GradientBoost are slightly ahead, but not much.\n",
    "\n",
    "**In short**\n",
    "All four algorithhms perform more or less the same with this data - however we can see very different training and testing times. They all have a lot of parameters that we can tune to make them much better than they are in this Notebook (depth of Decision Trees, maximum number of leaf nodes, minimum number of samples represented in each node, **learning rate** in AdaBoost and GradientBoost... etc.\n",
    "\n",
    "#### Use the cell below to play with Gradient Boosting and see if you can achieve better results\n",
    "Change the parameters as you like and see how the performance changes.\n",
    "\n",
    "Check the documentation [here to better understand the parameters](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) that Gradient Boost uses. As with RandomForest, many ot them relate to the underlying Decition Trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradBoost = GradientBoostingClassifier(\n",
    "    n_estimators=10,     # We will use again 100 DecisionTreeClassifiers, FEEL FREE TO CHANGE THIS\n",
    "    learning_rate=0.5,    # FEEL FREE TO CHANGE THIS TO TEST FOR BETTER RESULTS\n",
    "    subsample=1,        # Each new classifier will be trained on 100% of the data (no bagging). If it's less than 1 (100%), we will be doing a STOCHASTIC gradient boosting\n",
    "    loss='deviance', \n",
    "    criterion='friedman_mse', \n",
    "    min_samples_split=2, \n",
    "    min_samples_leaf=1, \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    max_depth=3, \n",
    "    min_impurity_decrease=0.0, \n",
    "    min_impurity_split=None, \n",
    "    init=None, \n",
    "    random_state=None, \n",
    "    max_features=None, \n",
    "    verbose=0, \n",
    "    max_leaf_nodes=None, \n",
    "    warm_start=False, \n",
    "    validation_fraction=0.1, \n",
    "    n_iter_no_change=None, \n",
    "    tol=0.0001, \n",
    "    ccp_alpha=0.0\n",
    ")\n",
    "print('Starting...')\n",
    "gb_score = cross_validate(\n",
    "    gradBoost, X, y, cv=5, scoring=metrics\n",
    ")\n",
    "print('Done')\n",
    "print_results(gb_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting? Let's experiment\n",
    "\n",
    "As a test, we will sub-sample our original data to make the two classes perfectly balanced and we will re-train our models using **just 2 features** (you can select which 2 down below). And then, we will plot the decision boundaries of the classifiers.\n",
    "\n",
    "In the plots below the blue areas are places where the classifier predicted that it will rain - and the red ones it is where it decided that it will not rain.\n",
    "\n",
    "The classifiers with many small areas are likely to be more overfitted (creating a blue areas for every single rain sample), while the ones with large and well defined blue areas are likely to generalise better.\n",
    "\n",
    "**There are not conclusive results**, just for me to show how we can plot this with our current data. Don't even think that we can extrapolate this and state: AdaBoost overfits less than RandomForest, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting decision regions:\n",
    "# CHOOSE ANY TWO INPUT FEATURES TO SEE HOW THE CLASSIFIERS DECIDED THE DECISION BOUNDARIES BETWEEN THEM!\n",
    "x_axis = 'temperaturemin'\n",
    "y_axis = 'temperaturemax'\n",
    "\n",
    "\n",
    "\n",
    "# Sub-sampling\n",
    "sample_data = df[df['rain'] == '0'].sample(len(df[df['rain'] == '1']))\n",
    "sample_data = sample_data.append(df[df['rain'] == '1']).sample(frac=1)\n",
    "\n",
    "# I will fit the classifiers using all of the data, so later we will plot the decision boundaries for the entire dataset:\n",
    "classifiers = [baggingDT, randomForest, adaBoost, gradBoost]\n",
    "for c in classifiers:\n",
    "    c.fit(sample_data[[x_axis, y_axis]], sample_data[target_feature_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X[x_axis].min() - 1, X[x_axis].max() + 1\n",
    "y_min, y_max = X[y_axis].min() - 1, X[y_axis].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "f, axarr = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(25, 15))\n",
    "plt.set_cmap('coolwarm')\n",
    "\n",
    "for idx, clf, tt in zip(product([0, 1], [0, 1]),\n",
    "                        classifiers,\n",
    "                        ['Bagging', 'Random Forest', 'AdaBoost', 'Gradient Boost']):\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.2)\n",
    "    axarr[idx[0], idx[1]].scatter(sample_data[x_axis], sample_data[y_axis], c=sample_data[target_feature_name], s=8)\n",
    "    axarr[idx[0], idx[1]].set_title(tt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Here is where you will need to get your computer science skills and get the dependencies installed.\n",
    "# In a Mac, I just had to `brew install libomp`. It might be more complicated in other systems.\n",
    "\n",
    "# As XGBoost becomes more and more standard, we expect the installation to become easier.\n",
    "\n",
    "# NOTE: THIS PART IS COMPLETELY OPTIONAL, NOT PART OF THE COURSE. You don't need to know the XGBoost library \n",
    "#(just the theory of the slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DMatrix is like Pandas DataFrame. We need to create it in order to train an XGBoost model with it:\n",
    "dmatrix = xgb.DMatrix(X, label=y)\n",
    "\n",
    "'''\n",
    "    Below I add some code cells designed to make you understand how DMatrices are used, even if you didn't manage to install \n",
    "    XGBoost in your machine (but maybe in the future in some work computer it'll be already installed so you can have some \n",
    "    knowledge to get started)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmatrix.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Num rows:', dmatrix.num_row(), 'Num cols:', dmatrix.num_col())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with XGBoost itself. First, we define a dictionary with the parameters of our model:\n",
    "model_parameters = {\n",
    "#     'n_estimators': 50, -> We can either set n_estimators like in Sklearn, or nb_rounds (I have it below)\n",
    "    'max_depth': 2,\n",
    "    'eta': 1.0,   # This is the learning rate! (As in Sklearn's GradientBoost)\n",
    "    'objective': 'binary:hinge', # There are many objective functions, depending on the type of the target variable and the metrics we want to evaluate it against \n",
    "    'eval_metric': ['auc'], # This part is very different to Sklearn. You will need to get familiar with many other metrics\n",
    "#     'lambda': .1,\n",
    "    'nthread': 4, # my computer has 4 cores - change to maximise the speed according to your computer cores\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_rounds = 50 #Number of boosting rounds!\n",
    "xgb_model = xgb.cv(\n",
    "    model_parameters, \n",
    "    dmatrix,\n",
    "    nb_rounds,\n",
    "    nfold=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will train a XGBoost model using the train method (much like the fit method in Sklearn)\n",
    "model = xgb.train(model_parameters, dmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And XGBoost has a function to see the feature importances:\n",
    "xgb.plot_importance(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And that's the last thing for today...\n",
    "\n",
    "Do you remember that we could use DecisionTrees and RandomForest to do some feature selection? \n",
    "\n",
    "Well all of the ensembles based on DecisionTrees* can do this too!! (as we just saw with XGBoost)\n",
    "\n",
    "(Apart from Bagging, which is not implemented in Sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in classifiers:\n",
    "    c.fit(X, y)\n",
    "    if hasattr(c, 'feature_importances_'):\n",
    "        print(c)\n",
    "        imp_feat = sorted(zip(input_features, c.feature_importances_), key=lambda x: -x[1])[:5]\n",
    "        for f, i in imp_feat:\n",
    "            print(f, '\\t', round(i, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### They all seem to agree, more or less :)\n",
    "\n",
    "# Learning Exercises:\n",
    "\n",
    "* The learning exercises are already laid down in the code comments throughout the notebook: Can you tune these ensembles to improve the performances I achieved? Tweak the different attributes of each method to maximise it. Maybe even run some Grid/Random optimiser\n",
    "* In the decision boundaries part: Can you reuse this code and show the decision boundaries for other classifiers, trained with 2 features? (e.g. SVM, KNN, DecisionTree, NaiveBayes, etc, etc). You can learn a lot about how all of the classifiers, and their variations (kernels, parameters), work. Can be a great reviewing exercise\n",
    "* For those of you who managed to get XGBoost installed: can you match the ROC AUC that we got in the other ensembles? (if you do, let me know)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
