{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Synthetic Data\n",
    "\n",
    "This week we will start by creating some synthetic data that we will use to illustrate the different clustering algorithms that we have seen this week. Sklearn has a very neat way of creating synthetic data for clustering.\n",
    "\n",
    "**You can click and drag the 3D charts**, enjoy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these constants to modify the experiments below!\n",
    "NUM_CLUSTERS = 7\n",
    "NUM_SAMPLES = 300\n",
    "\n",
    "X, y = make_blobs(\n",
    "    n_samples=NUM_SAMPLES,\n",
    "    n_features=3, # We will add just 3 features so we can plot the data in a 3D plot! :)\n",
    "    centers=NUM_CLUSTERS,   # blobs or clusters (to see how well our clustering methods recognise them later)\n",
    "    cluster_std=0.5,\n",
    "    shuffle=True,\n",
    "    random_state=0\n",
    ")\n",
    "# We won't use \"y\" since we are in unsupervised learning, we don't really need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X, columns=['feature1', 'feature2', 'feature3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f''' Below you should see a 3D graph, which you can rotate with the synthetic data\n",
    "spread in {NUM_CLUSTERS} clear clusters ''')\n",
    "fig = px.scatter_3d(df, x='feature1', y='feature2', z='feature3')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the clustering algorithms we have seen this week:\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# MiniBatchKMeans is a faster implementation of KMeans which sacrifices a bit of \n",
    "# clustering performance to get faster results. Try it out if you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_random = KMeans(\n",
    "    n_clusters=NUM_CLUSTERS,\n",
    "    init='random',\n",
    "    n_init=1, # We just want to run the algorithm once - not several times and get the best results!\n",
    "    max_iter=100,\n",
    ")\n",
    "kmeans_random_clusters = kmeans_random.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kmeans_random_clusters'] = kmeans_random_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df, x='feature1', y='feature2', z='feature3', \n",
    "                    color='kmeans_random_clusters', color_continuous_scale='Rainbow')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how K-means with random initialisation got some of the clusters wrong (at least in my execution).\n",
    "Let's now try with K-means++:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_plusplus = KMeans(\n",
    "    n_clusters=NUM_CLUSTERS,\n",
    "    init='k-means++',\n",
    "    n_init=1, # We just want to run the algorithm once - not several times and get the best results!\n",
    "    max_iter=100,\n",
    ")\n",
    "kmeans_plusplus_clusters = kmeans_plusplus.fit_predict(X)\n",
    "df['kmeans_plusplus_clusters'] = kmeans_plusplus_clusters\n",
    "fig = px.scatter_3d(df, x='feature1', y='feature2', z='feature3', \n",
    "                    color='kmeans_plusplus_clusters', color_continuous_scale='Rainbow')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better now! (**there are no blobs split in multi-colours**). And it seems like GMM is also giving good results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(\n",
    "    n_components=NUM_CLUSTERS,\n",
    "    n_init=1 # Same as before\n",
    ")\n",
    "gmm_clusters = gmm.fit_predict(X)\n",
    "df['gmm_clusters'] = gmm_clusters\n",
    "fig = px.scatter_3d(df, x='feature1', y='feature2', z='feature3', \n",
    "                    color='gmm_clusters', color_continuous_scale='Rainbow')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to observe how Agglomerative Clustering changes its results:\n",
    "DISTANCE_THRESHOLD = 5 # With a smaller distance we will get better results\n",
    "\n",
    "\n",
    "agg_clustering = AgglomerativeClustering(\n",
    "    n_clusters=None, # We don't need this, just the distance threshold\n",
    "    linkage='single', # Try other linkages as we saw in the session: linkage{‘ward’, ‘complete’, ‘average’, ‘single’}\n",
    "    distance_threshold=DISTANCE_THRESHOLD,\n",
    ")\n",
    "agg_clusters = agg_clustering.fit_predict(X)\n",
    "df['agg_clusters'] = agg_clusters\n",
    "fig = px.scatter_3d(df, x='feature1', y='feature2', z='feature3', \n",
    "                    color='agg_clusters', color_continuous_scale='Rainbow')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Clustering Images Data\n",
    "\n",
    "Just as we did in the Introduction to Machine Learning course, we will start by using the most standard dataset. In supervised classification it was the Iris Dataset, and here is the MNIST dataset.\n",
    "\n",
    "This dataset consists of a training set of 60k images of hand-written digits. The digits are from 0 to 9. So our target feature has 10 classes. Additionally, it comes with a testing set of another 10k images.\n",
    "\n",
    "Each image is a 28x28 pixels, greyscale one. Each pixel has a value between 0 and 255 that represents how dark or clear the value of that pixel is.\n",
    "\n",
    "We will try to group the images that correspond to the same digit (without using the labels, just the pixels!!). This time we will also use KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "images = digits['data'] / 255 # A quick normalisation so all pixel values are between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Insert any number between 0 and 60k to visualise one training data\n",
    "record. I put 234 for example. BUT TRY SOME OTHER:\n",
    "'''\n",
    "SAMPLE_RECORD_NUMBER = 234\n",
    "\n",
    "print('This looks like a six:')\n",
    "plt.gray() \n",
    "plt.matshow(digits.images[SAMPLE_RECORD_NUMBER]) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train our KNN model (without a target feature!)\n",
    "knn = NearestNeighbors(\n",
    "    n_neighbors=100, # There are some more parameters you could tweak, check documentation\n",
    ")\n",
    "knn.fit(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now check the nearest neighbors to our example above:\n",
    "distances, neighbors = knn.kneighbors([images[SAMPLE_RECORD_NUMBER].reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distances contain the Euclidean distances in an array, and neighbors contain the indices\n",
    "# of the neighbor samples. Both are sorted by distance, in ascending order.\n",
    "\n",
    "'''Let's plot the top 5 nearest neighbors of this instance, which are the last 5 indices'''\n",
    "\n",
    "for index in range(1, 6):\n",
    "    plt.gray() \n",
    "    plt.matshow(digits.images[neighbors[0][index]]) \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yeah! \n",
    "Seems like they are all images of the number 6. KNN is working here!!\n",
    "\n",
    "## Let's now try the MiniBatchKMeans: (faster KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_minibatch = (\n",
    "    n_clusters=10, # We know the images are for the digits 0 to 9, so we should have 10 clusters!\n",
    "    init='k-means++',\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    ")\n",
    "kmeans_minibatch_clusters = kmeans_minibatch.fit_predict(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_minibatch_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "for cluster_id, image in zip(kmeans_minibatch_clusters, images):\n",
    "    if cluster_id in clusters:\n",
    "        clusters[cluster_id].append(image)\n",
    "    else:\n",
    "        clusters[cluster_id] = [image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one cluster to display some samples in it!\n",
    "# The cluster_id has nothing to do with the digits' value, but in each cluster,\n",
    "# very similar images should be together: (select any number from 0 to 9)\n",
    "CLUSTER_ID_TO_DISPLAY = 4\n",
    "\n",
    "grid_size = 5\n",
    "\n",
    "fig, axes = plt.subplots(grid_size, grid_size, sharex=True, sharey=True,figsize=(15, 15))\n",
    "\n",
    "for row in axes:\n",
    "    for i in range(grid_size):\n",
    "        random_index = random.randint(0, len(clusters[CLUSTER_ID_TO_DISPLAY])-1)\n",
    "        row[i].matshow(clusters[CLUSTER_ID_TO_DISPLAY][random_index].reshape(8, 8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning exercises:\n",
    "\n",
    "* Can you apply more clustering algorithms to the digit images to improve performance?\n",
    "\n",
    "* Can you tune the agglomerative clustering algorithm to work better?\n",
    "\n",
    "* There are ways to plot a dendrogram with the scipy library and also with the plotly library, can you make it happen to see how your agglomerative clustering\n",
    "\n",
    "* And as always, play with all of the constant parameters I specified in capitals to see and understand how the outputs change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
